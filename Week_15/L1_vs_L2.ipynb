{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa6cc3f-ae19-4b66-b84d-5a73f41a11c7",
   "metadata": {},
   "source": [
    "# L1 and L2 Regularization: Mathematical Foundations and Practical Implementation\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### L1 Regularization (Lasso)\n",
    "\n",
    "#### Objective Function\n",
    "$\\min_w \\left[ \\text{Loss}(w) + \\lambda \\sum_{j=1}^p |w_j| \\right]$\n",
    "\n",
    "Where:\n",
    "- $w_j$: Model coefficients\n",
    "- $\\lambda$: Regularization strength\n",
    "- $\\text{Loss}(w)$: Original model loss function\n",
    "\n",
    "#### Key Properties\n",
    "1. **Penalty Calculation**:\n",
    "   $L1\\_\\text{penalty} = \\lambda \\sum_{j=1}^p |w_j|$\n",
    "\n",
    "2. **Geometric Interpretation**:\n",
    "   - Constraint region forms a diamond (octahedron)\n",
    "   - Encourages sparse solutions\n",
    "   - Forces some coefficients to exactly zero\n",
    "\n",
    "### L2 Regularization (Ridge)\n",
    "\n",
    "#### Objective Function\n",
    "$\\min_w \\left[ \\text{Loss}(w) + \\lambda \\sum_{j=1}^p w_j^2 \\right]$\n",
    "\n",
    "Where:\n",
    "- $w_j$: Model coefficients\n",
    "- $\\lambda$: Regularization strength\n",
    "- $\\text{Loss}(w)$: Original model loss function\n",
    "\n",
    "#### Key Properties\n",
    "1. **Penalty Calculation**:\n",
    "   $L2\\_\\text{penalty} = \\lambda \\sum_{j=1}^p w_j^2$\n",
    "\n",
    "2. **Geometric Interpretation**:\n",
    "   - Constraint region forms a sphere\n",
    "   - Uniformly shrinks coefficients\n",
    "   - Never forces coefficients to exactly zero\n",
    "\n",
    "## Comprehensive Python Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "class RegularizationComparison:\n",
    "    def __init__(self, n_features=20, n_informative=5, noise=0.1):\n",
    "        # Generate synthetic data\n",
    "        X, y = make_regression(\n",
    "            n_samples=200, \n",
    "            n_features=n_features, \n",
    "            n_informative=n_informative, \n",
    "            noise=noise, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Split data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "    \n",
    "    def compare_regularization(self, alphas=[0.01, 0.1, 1, 10]):\n",
    "        results = {\n",
    "            'Lasso': [],\n",
    "            'Ridge': []\n",
    "        }\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            # Lasso Regression\n",
    "            lasso = Lasso(alpha=alpha, random_state=42)\n",
    "            lasso.fit(self.X_train_scaled, self.y_train)\n",
    "            results['Lasso'].append({\n",
    "                'alpha': alpha,\n",
    "                'coefficients': lasso.coef_,\n",
    "                'score': lasso.score(self.X_test_scaled, self.y_test)\n",
    "            })\n",
    "            \n",
    "            # Ridge Regression\n",
    "            ridge = Ridge(alpha=alpha, random_state=42)\n",
    "            ridge.fit(self.X_train_scaled, self.y_train)\n",
    "            results['Ridge'].append({\n",
    "                'alpha': alpha,\n",
    "                'coefficients': ridge.coef_,\n",
    "                'score': ridge.score(self.X_test_scaled, self.y_test)\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Visualization\n",
    "def plot_coefficient_paths(results):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Lasso Coefficients\n",
    "    plt.subplot(1, 2, 1)\n",
    "    lasso_coeffs = [res['coefficients'] for res in results['Lasso']]\n",
    "    plt.title('Lasso Coefficient Paths')\n",
    "    plt.xlabel('Regularization Strength (λ)')\n",
    "    plt.ylabel('Coefficient Value')\n",
    "    plt.plot(np.log([r['alpha'] for r in results['Lasso']]), \n",
    "             lasso_coeffs, marker='o')\n",
    "    \n",
    "    # Ridge Coefficients\n",
    "    plt.subplot(1, 2, 2)\n",
    "    ridge_coeffs = [res['coefficients'] for res in results['Ridge']]\n",
    "    plt.title('Ridge Coefficient Paths')\n",
    "    plt.xlabel('Regularization Strength (λ)')\n",
    "    plt.ylabel('Coefficient Value')\n",
    "    plt.plot(np.log([r['alpha'] for r in results['Ridge']]), \n",
    "             ridge_coeffs, marker='o')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Demonstration\n",
    "comparison = RegularizationComparison()\n",
    "results = comparison.compare_regularization()\n",
    "plot_coefficient_paths(results)\n",
    "```\n",
    "\n",
    "## Mathematical Insights\n",
    "\n",
    "### Regularization Strength Analysis\n",
    "\n",
    "#### L1 Regularization Loss Function\n",
    "$J(w) = \\text{MSE} + \\lambda \\sum_{j=1}^p |w_j|$\n",
    "\n",
    "#### L2 Regularization Loss Function\n",
    "$J(w) = \\text{MSE} + \\lambda \\sum_{j=1}^p w_j^2$\n",
    "\n",
    "### Sparsity Probability\n",
    "- **L1 Regularization**: \n",
    "  $P(\\text{Coefficient} = 0) \\propto \\lambda$\n",
    "- **L2 Regularization**: \n",
    "  $P(\\text{Coefficient} \\approx 0) \\propto \\frac{1}{\\lambda}$\n",
    "\n",
    "## Practical Guidelines\n",
    "\n",
    "### When to Use L1 (Lasso)\n",
    "- High-dimensional datasets\n",
    "- Feature selection required\n",
    "- Sparse model desired\n",
    "- Many irrelevant features\n",
    "\n",
    "### When to Use L2 (Ridge)\n",
    "- All features potentially relevant\n",
    "- Multicollinearity present\n",
    "- Smooth coefficient reduction needed\n",
    "- Stable predictions desired\n",
    "\n",
    "### Hybrid Approach: Elastic Net\n",
    "Combines L1 and L2 penalties:\n",
    "$\\text{Loss} + \\lambda_1 \\sum |w_j| + \\lambda_2 \\sum w_j^2$\n",
    "\n",
    "## Key Takeaways\n",
    "1. L1 creates sparse models\n",
    "2. L2 shrinks coefficients uniformly\n",
    "3. Regularization strength is crucial\n",
    "4. Always cross-validate to find optimal parameters\n",
    "\n",
    "**Pro Tip**: Experiment with different regularization techniques and strengths to find the best model performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
