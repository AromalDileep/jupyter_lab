{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e5b165-2b7c-43ee-8cb9-6381f9a7fac2",
   "metadata": {},
   "source": [
    "### **Wrapper Methods in Feature Selection**\n",
    "**Definition**:  \n",
    "Wrapper methods are feature selection techniques that evaluate subsets of features by training and testing a machine learning model. They aim to find the best-performing feature subset for the model by iteratively adding or removing features and measuring the model's performance. These methods are computationally intensive but usually provide better results than simpler techniques like filter methods.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Techniques in Wrapper Methods**\n",
    "1. **Forward Selection**:  \n",
    "   - Starts with no features.\n",
    "   - Adds features one by one based on their contribution to model performance.\n",
    "   - Stops when no significant improvement is observed.\n",
    "\n",
    "2. **Backward Elimination**:  \n",
    "   - Starts with all features.\n",
    "   - Removes one feature at a time based on the least impact on model performance.\n",
    "   - Stops when further removal reduces model performance.\n",
    "\n",
    "3. **Recursive Feature Elimination (RFE)**:  \n",
    "   - Uses a model (like a linear regression or tree-based model) to rank features based on importance.\n",
    "   - Iteratively removes the least important features and retrains the model.\n",
    "   - Outputs the best subset of features.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works (Step-by-Step)**\n",
    "\n",
    "1. Select a machine learning model.\n",
    "2. Define a metric to evaluate model performance (e.g., accuracy, F1-score, RMSE).\n",
    "3. Use one of the above techniques to iteratively select or eliminate features.\n",
    "4. Test the model performance for each subset of features.\n",
    "5. Select the subset with the best performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example with Code**\n",
    "Letâ€™s demonstrate **Recursive Feature Elimination (RFE)** using Python and scikit-learn:\n",
    "\n",
    "#### **Code Example**\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e31f48b-56b0-494b-8a4a-04c0413894e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "['petal length (cm)', 'petal width (cm)']\n",
      "\n",
      "Feature Rankings (1 is best):\n",
      "[2 3 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Target\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Apply RFE for feature selection\n",
    "rfe = RFE(estimator=model, n_features_to_select=2)  # Select 2 best features\n",
    "X_selected = rfe.fit_transform(X, y)\n",
    "\n",
    "# Print results\n",
    "print(\"Selected Features:\")\n",
    "print([f for f, selected in zip(data.feature_names, rfe.support_) if selected])\n",
    "\n",
    "print(\"\\nFeature Rankings (1 is best):\")\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94748fa-726b-4ff9-b9b6-aeba0c011e42",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### **Explanation of Code**:\n",
    "1. **Dataset**: The Iris dataset has 4 features (e.g., petal length, sepal width).\n",
    "2. **Model**: A Random Forest classifier is chosen as the estimator for RFE.\n",
    "3. **RFE**: Iteratively removes the least important features and selects the top 2 based on feature importance.\n",
    "4. **Output**: \n",
    "   - Prints the names of selected features.\n",
    "   - Shows the rankings for all features (lower rank = better feature).\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of Wrapper Methods**\n",
    "- Accounts for feature interactions.\n",
    "- Provides subsets tailored to the specific machine learning model.\n",
    "\n",
    "### **Disadvantages**\n",
    "- Computationally expensive, especially for large datasets.\n",
    "- Prone to overfitting if not used carefully.\n",
    "\n",
    "Let me know if you need additional explanations or examples for the other techniques!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
