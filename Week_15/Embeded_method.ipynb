{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0f71bea-e193-45a0-8328-15cea3bea05c",
   "metadata": {},
   "source": [
    "**Embedded Methods in Feature Selection**\n",
    "\n",
    "**Definition**:  \n",
    "Embedded methods are feature selection techniques that are integrated into the training process of a machine learning model. These methods use the model’s own learning algorithm to decide which features are most important, combining the benefits of filter and wrapper methods. They are computationally efficient and often yield highly relevant features.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Techniques in Embedded Methods**\n",
    "\n",
    "1. **LASSO (L1 Regularization)**:  \n",
    "   - Adds a penalty to the loss function of a model to shrink coefficients of less important features to zero, effectively removing them.\n",
    "   - Commonly used in linear models (e.g., Logistic Regression, Linear Regression).\n",
    "\n",
    "2. **Tree-Based Feature Selection**:  \n",
    "   - Decision tree-based models (e.g., Random Forest, Gradient Boosting) calculate feature importance during training based on metrics like Gini Impurity or Information Gain.\n",
    "\n",
    "3. **Elastic Net Regularization**:  \n",
    "   - Combines L1 (LASSO) and L2 (Ridge) penalties to balance between feature selection and coefficient shrinkage.\n",
    "\n",
    "4. **Regularized Logistic Regression**:  \n",
    "   - Logistic regression with regularization (L1 or Elastic Net) selects features while building the model.\n",
    "\n",
    "---\n",
    "\n",
    "**How It Works (Step-by-Step)**\n",
    "\n",
    "1. Select a machine learning algorithm with built-in feature selection capability (e.g., LASSO or tree-based models).  \n",
    "2. Train the model on the dataset.  \n",
    "3. The algorithm automatically calculates feature importance or removes irrelevant features during the training process.  \n",
    "4. Extract the most important features based on the algorithm’s output.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Code Example**\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9fd710c-14b8-4a74-a479-6b008bba4530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance (LASSO):\n",
      "             Feature  Coefficient\n",
      "0  sepal length (cm)     0.000000\n",
      "1   sepal width (cm)    -0.000000\n",
      "2  petal length (cm)     0.314092\n",
      "3   petal width (cm)     0.375744\n",
      "\n",
      "Selected Features:\n",
      "['petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load Dataset\n",
    "data = load_iris()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Target\n",
    "\n",
    "# Standardize Features (LASSO works better with normalized data)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply LASSO for Feature Selection\n",
    "lasso = Lasso(alpha=0.1)  # Regularization strength\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Extract Feature Importance\n",
    "feature_coefficients = pd.DataFrame({\n",
    "    'Feature': data.feature_names,\n",
    "    'Coefficient': lasso.coef_\n",
    "})\n",
    "\n",
    "# Selected Features\n",
    "selected_features = feature_coefficients[feature_coefficients['Coefficient'] != 0]['Feature']\n",
    "\n",
    "# Print Results\n",
    "print(\"Feature Importance (LASSO):\")\n",
    "print(feature_coefficients)\n",
    "\n",
    "print(\"\\nSelected Features:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c2084c-ede1-4809-8fa4-79b1f1d60d91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Explanation of Code**:\n",
    "1. **Dataset**: The Iris dataset is used, with four features (e.g., petal length, sepal width).\n",
    "2. **Standardization**: LASSO requires normalized data for better performance.\n",
    "3. **Model**: LASSO regression is applied with an alpha value of 0.1 for regularization.\n",
    "4. **Feature Importance**: Coefficients of features are computed, with zero coefficients indicating irrelevant features.\n",
    "5. **Output**: Selected features and their coefficients are displayed.\n",
    "\n",
    "---\n",
    "\n",
    "**Advantages of Embedded Methods**  \n",
    "- Computationally efficient compared to wrapper methods.  \n",
    "- Integrated into the model’s training, reducing the need for external steps.  \n",
    "- Accounts for feature interactions (especially with tree-based methods).  \n",
    "\n",
    "**Disadvantages**  \n",
    "- Dependent on the algorithm used (model-specific).  \n",
    "- May not work well with all datasets or when irrelevant features dominate.  \n",
    "\n",
    "Let me know if you'd like further clarification or examples!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
