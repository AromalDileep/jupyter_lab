{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a67bcc5-2339-41fe-b715-9b0ea57df114",
   "metadata": {},
   "source": [
    "Here’s a detailed explanation of feature selection techniques in machine learning, followed by a Python notebook layout for practical implementation. \n",
    "\n",
    "---\n",
    "\n",
    "### **Feature Selection Techniques**\n",
    "Feature selection involves selecting the most important variables that contribute to predicting the target variable while removing redundant or irrelevant features. It improves model performance, reduces overfitting, and makes models faster and easier to interpret.\n",
    "\n",
    "#### **Types of Feature Selection Methods**\n",
    "\n",
    "1. **Filter Methods**  \n",
    "   - Select features based on statistical measures like correlation or variance.\n",
    "   - Techniques include:\n",
    "     - **Variance Thresholding**\n",
    "     - **Correlation Matrix**\n",
    "     - **Mutual Information**\n",
    "\n",
    "2. **Wrapper Methods**  \n",
    "   - Use a predictive model to evaluate the performance of feature subsets.\n",
    "   - Techniques include:\n",
    "     - **Recursive Feature Elimination (RFE)**\n",
    "     - **Forward Selection**\n",
    "     - **Backward Elimination**\n",
    "\n",
    "3. **Embedded Methods**  \n",
    "   - Feature selection is part of the model training process.\n",
    "   - Techniques include:\n",
    "     - **LASSO (L1 Regularization)**\n",
    "     - **Decision Tree-based feature importance**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf956cc6-3b79-4856-b7c3-f14f9e892744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "Top 5 Features by Mutual Information: ['MedInc', 'AveRooms', 'AveOccup', 'Latitude', 'Longitude']\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, RFE, VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load California Housing Dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# 1. Filter Method: Variance Threshold\n",
    "def filter_variance_threshold(X, threshold=0.01):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    X_var = selector.fit_transform(X)\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    print(f\"Selected Features: {list(selected_features)}\")\n",
    "    return pd.DataFrame(X_var, columns=selected_features)\n",
    "\n",
    "X_var_filtered = filter_variance_threshold(X)\n",
    "\n",
    "# 2. Filter Method: Mutual Information\n",
    "def filter_mutual_information(X, y, k=5):\n",
    "    selector = SelectKBest(score_func=mutual_info_regression, k=k)\n",
    "    X_mutual = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    print(f\"Top {k} Features by Mutual Information: {list(selected_features)}\")\n",
    "    return pd.DataFrame(X_mutual, columns=selected_features)\n",
    "\n",
    "X_mutual_filtered = filter_mutual_information(X, y, k=5)\n",
    "\n",
    "# 3. Wrapper Method: Recursive Feature Elimination (RFE)\n",
    "def wrapper_rfe(X, y, n_features_to_select=5):\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    rfe = RFE(model, n_features_to_select=n_features_to_select)\n",
    "    rfe.fit(X, y)\n",
    "    selected_features = X.columns[rfe.support_]\n",
    "    print(f\"Top {n_features_to_select} Features by RFE: {list(selected_features)}\")\n",
    "    return pd.DataFrame(X[selected_features])\n",
    "\n",
    "X_rfe_filtered = wrapper_rfe(X, y)\n",
    "\n",
    "# 4. Embedded Method: Lasso (L1 Regularization)\n",
    "def embedded_lasso(X, y):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    lasso = LassoCV(cv=5, random_state=42)\n",
    "    lasso.fit(X_scaled, y)\n",
    "    selected_features = X.columns[lasso.coef_ != 0]\n",
    "    print(f\"Selected Features by Lasso: {list(selected_features)}\")\n",
    "    return pd.DataFrame(X[selected_features])\n",
    "\n",
    "X_lasso_filtered = embedded_lasso(X, y)\n",
    "\n",
    "# 5. Tree-Based Feature Importance\n",
    "def tree_based_feature_importance(X, y):\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    model.fit(X, y)\n",
    "    importance = pd.DataFrame(\n",
    "        {\"Feature\": X.columns, \"Importance\": model.feature_importances_}\n",
    "    ).sort_values(by=\"Importance\", ascending=False)\n",
    "    print(\"Feature Importance (Tree-based):\")\n",
    "    print(importance)\n",
    "    return importance\n",
    "\n",
    "feature_importance = tree_based_feature_importance(X, y)\n",
    "\n",
    "# Output and Summary\n",
    "print(\"\\nSummary:\")\n",
    "print(\"Filtered Features (Variance Threshold):\", list(X_var_filtered.columns))\n",
    "print(\"Filtered Features (Mutual Information):\", list(X_mutual_filtered.columns))\n",
    "print(\"Filtered Features (RFE):\", list(X_rfe_filtered.columns))\n",
    "print(\"Filtered Features (Lasso):\", list(X_lasso_filtered.columns))\n",
    "# Test one function directly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed78a58-6bc8-4cc5-ac5b-bbca3be5f5ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Steps in Notebook**\n",
    "\n",
    "1. **Load Libraries and Dataset**:\n",
    "   - Use datasets like Iris or Boston Housing.\n",
    "   - Preprocess features using scaling if necessary.\n",
    "\n",
    "2. **Filter Methods**:\n",
    "   - **Variance Threshold** to eliminate low-variance features.\n",
    "   - **Mutual Information** to find the features most related to the target.\n",
    "\n",
    "3. **Wrapper Methods**:\n",
    "   - Apply **RFE** with a classifier like Random Forest.\n",
    "\n",
    "4. **Embedded Methods**:\n",
    "   - Use **LASSO** for feature selection through regularization.\n",
    "   - Evaluate **Tree-based feature importance**.\n",
    "\n",
    "5. **Summary**:\n",
    "   - Print selected features from each technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6e774-2d65-4778-b9cc-b26f1e0e8d32",
   "metadata": {},
   "source": [
    "# Simple explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab916e64-d390-4723-b3d9-e158df4e7d87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1. Variance Thresholding**\n",
    "### **What it is**:\n",
    "Variance Thresholding removes features (columns) that don't change much in your data. If a feature has the same value for most rows (or has very little variation), it won't help the model learn anything useful.\n",
    "\n",
    "### **Example**:\n",
    "Imagine you have a dataset of student scores in different subjects:\n",
    "\n",
    "| Student | Math | English | Science |\n",
    "|---------|------|---------|---------|\n",
    "| A       | 85   | 90      | 75      |\n",
    "| B       | 85   | 92      | 76      |\n",
    "| C       | 85   | 88      | 74      |\n",
    "\n",
    "In this case:\n",
    "- **Math** has no variation (all students have the same score: 85), so its variance is 0.\n",
    "- **English** and **Science** have different values and thus have variance.\n",
    "\n",
    "### **Result**:\n",
    "Since the **Math** feature has no variation, we would **remove it** using Variance Thresholding because it won't help the model make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a7ceb4-a726-4535-96c3-6fac28aea983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Example data\n",
    "import numpy as np\n",
    "X = np.array([[0, 2, 0, 3],\n",
    "              [0, 1, 4, 3],\n",
    "              [0, 1, 1, 3]])\n",
    "\n",
    "# Apply VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.5)  # Set variance threshold\n",
    "X_selected = selector.fit_transform(X)\n",
    "\n",
    "print(\"Selected Features:\\n\", X_selected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b9fcc-fa0f-46bb-a381-1600abe90d8c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **2. Correlation Matrix**\n",
    "### **What it is**:\n",
    "The correlation matrix shows how much two features are related to each other. If two features are very similar (correlated), you might not need both, because they give the same information.\n",
    "\n",
    "### **Example**:\n",
    "Imagine you have a dataset about house prices:\n",
    "\n",
    "| House | Size (sq ft) | Bedrooms | Price |\n",
    "|-------|--------------|----------|-------|\n",
    "| 1     | 1200         | 3        | 300000|\n",
    "| 2     | 1500         | 4        | 350000|\n",
    "| 3     | 1800         | 4        | 400000|\n",
    "\n",
    "- **Size** and **Bedrooms** are related. Bigger houses tend to have more bedrooms.\n",
    "- A **high correlation** means these two features give similar information.\n",
    "\n",
    "### **Result**:\n",
    "If the correlation between **Size** and **Bedrooms** is high (e.g., above 0.9), you can **remove one** (e.g., **Bedrooms**) to avoid redundancy. This keeps the model simpler and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3920afd0-9194-4430-b476-9437ef3256da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4],\n",
    "    'B': [2, 4, 6, 8],\n",
    "    'C': [1, 1, 2, 2],\n",
    "    'D': [10, 20, 30, 40]\n",
    "})\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "\n",
    "# Remove features with high correlation (e.g., r > 0.8)\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "print(\"Features to drop:\", to_drop)\n",
    "\n",
    "# Drop the features\n",
    "data_selected = data.drop(columns=to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291643f-25f8-4180-b0fd-fdc1c9cb41b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **3. Mutual Information**\n",
    "### **What it is**:\n",
    "Mutual Information tells us how much one feature helps in predicting the target (the thing you're trying to predict). Features that give a lot of useful information about the target are kept, while those that don’t are removed.\n",
    "\n",
    "### **Example**:\n",
    "Let's say you're trying to predict if someone buys a product (Yes or No) based on two features:\n",
    "1. **Age** (How old the person is)\n",
    "2. **Height** (How tall the person is)\n",
    "\n",
    "| Person | Age | Height | Bought Product |\n",
    "|--------|-----|--------|----------------|\n",
    "| 1      | 25  | 160    | Yes            |\n",
    "| 2      | 30  | 170    | No             |\n",
    "| 3      | 22  | 165    | Yes            |\n",
    "| 4      | 28  | 168    | No             |\n",
    "\n",
    "- **Age** might tell you something useful about whether a person buys the product (e.g., younger people tend to buy more).\n",
    "- **Height**, on the other hand, might not have any relationship with whether they buy the product.\n",
    "\n",
    "### **Result**:\n",
    "- **Age** has high Mutual Information with the target (whether they bought the product), so it’s kept.\n",
    "- **Height** has low Mutual Information with the target, so it might be **removed**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**:\n",
    "1. **Variance Thresholding**: Remove features that don’t change much (like a column where every value is the same).\n",
    "2. **Correlation Matrix**: Remove one of two features that are highly related to each other (like \"Size\" and \"Bedrooms\").\n",
    "3. **Mutual Information**: Keep features that help predict the target (like \"Age\" for predicting whether a person buys something).\n",
    "\n",
    "Would you like to see how these methods are implemented in a real dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a58198-03ad-4d2b-a7b0-e470dbb03113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "X = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4],\n",
    "    'B': [2, 4, 6, 8],\n",
    "    'C': [1, 1, 2, 2]\n",
    "})\n",
    "y = [0, 1, 0, 1]  # Target variable\n",
    "\n",
    "# Compute mutual information\n",
    "mi_scores = mutual_info_classif(X, y)\n",
    "\n",
    "# Display MI scores\n",
    "mi_scores = pd.Series(mi_scores, index=X.columns)\n",
    "print(\"Mutual Information Scores:\\n\", mi_scores)\n",
    "\n",
    "# Select features based on threshold\n",
    "selected_features = mi_scores[mi_scores > 0.1].index\n",
    "print(\"Selected Features:\", list(selected_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ded4c-28fb-4f0d-b231-5f1c0c424e7d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **4. ANOVA (Analysis of Variance)**\n",
    "### **What it is**:\n",
    "ANOVA is a statistical method that checks if there are any significant differences between the means of two or more groups. In feature selection, it's used to see how much a feature is related to the target variable. ANOVA tests if different values of a feature lead to different target values.\n",
    "\n",
    "### **How it works**:\n",
    "- ANOVA compares the mean of the target variable for each group (based on a feature) and checks if the differences between those means are statistically significant.\n",
    "- A **high p-value** indicates that the feature doesn't contribute much to predicting the target, and it can be **removed**. A **low p-value** means the feature is useful.\n",
    "\n",
    "### **Example**:\n",
    "Imagine you have a dataset where the target variable is whether a student passed or failed a course (binary: Pass/Fail). The feature is the **study hours**:\n",
    "\n",
    "| Student | Study Hours | Passed (Target) |\n",
    "|---------|-------------|-----------------|\n",
    "| A       | 2           | Fail            |\n",
    "| B       | 3           | Fail            |\n",
    "| C       | 8           | Pass            |\n",
    "| D       | 9           | Pass            |\n",
    "\n",
    "You want to know if the **study hours** are significantly different between the \"Pass\" and \"Fail\" groups.\n",
    "\n",
    "- ANOVA tests if the average number of study hours is significantly different between the \"Pass\" and \"Fail\" groups.\n",
    "\n",
    "### **Result**:\n",
    "- If **study hours** are significantly different between the \"Pass\" and \"Fail\" groups (low p-value), then **study hours** is a **useful feature**.\n",
    "- If the p-value is high, this feature doesn't help much and can be removed.\n",
    "\n",
    "### **Steps in Python**:\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097686ce-0889-4cde-99bc-6a69b30474a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "data = pd.DataFrame({\n",
    "    'Study Hours': [2, 3, 8, 9],\n",
    "    'Passed': ['Fail', 'Fail', 'Pass', 'Pass']\n",
    "})\n",
    "\n",
    "# Convert categorical target to numerical (0 = Fail, 1 = Pass)\n",
    "data['Passed'] = data['Passed'].map({'Fail': 0, 'Pass': 1})\n",
    "\n",
    "# Apply ANOVA F-test (used for classification)\n",
    "f_statistic, p_value = f_classif(data[['Study Hours']], data['Passed'])\n",
    "print(\"ANOVA p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b9e9d-d81a-458f-8227-83fdf351136d",
   "metadata": {},
   "source": [
    "### **When to use**:\n",
    "- **Classification problems** where the target is categorical.\n",
    "- Check the relationship between a continuous feature and a categorical target.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Chi-Square Test**\n",
    "### **What it is**:\n",
    "The **Chi-Square** test is used to measure how well a categorical feature matches with the target. It checks if there's a statistically significant association between the feature and the target variable.\n",
    "\n",
    "### **How it works**:\n",
    "- The test compares the observed frequency of each combination of feature and target to the expected frequency (what you'd expect if there was no relationship).\n",
    "- A **low p-value** indicates a significant relationship, and a **high p-value** means no relationship, suggesting the feature might be removed.\n",
    "\n",
    "### **Example**:\n",
    "Imagine you have a dataset of students and whether they passed or failed the course, but this time the **study method** is a categorical feature (e.g., \"Self-Study\" or \"Group Study\"):\n",
    "\n",
    "| Student | Study Method  | Passed (Target) |\n",
    "|---------|---------------|-----------------|\n",
    "| A       | Self-Study    | Fail            |\n",
    "| B       | Group Study   | Pass            |\n",
    "| C       | Self-Study    | Pass            |\n",
    "| D       | Group Study   | Fail            |\n",
    "\n",
    "You want to know if the **Study Method** is associated with whether a student passes or fails.\n",
    "\n",
    "### **Result**:\n",
    "- The **Chi-Square test** will tell you if the **Study Method** (Self-Study or Group Study) is related to the target (Pass or Fail).\n",
    "- If there's a strong relationship (low p-value), the feature is **useful**. If there's no relationship (high p-value), you may want to **drop the feature**.\n",
    "\n",
    "### **Steps in Python**:\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced88aa-0436-4c2a-ada6-ed5424b284a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example data\n",
    "data = pd.DataFrame({\n",
    "    'Study Method': ['Self-Study', 'Group Study', 'Self-Study', 'Group Study'],\n",
    "    'Passed': ['Fail', 'Pass', 'Pass', 'Fail']\n",
    "})\n",
    "\n",
    "# Convert categorical data to numerical\n",
    "encoder = LabelEncoder()\n",
    "data['Study Method'] = encoder.fit_transform(data['Study Method'])\n",
    "data['Passed'] = encoder.fit_transform(data['Passed'])\n",
    "\n",
    "# Apply Chi-Square test\n",
    "chi2_stat, p_value = chi2(data[['Study Method']], data['Passed'])\n",
    "print(\"Chi-Square p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ff6590-f42b-4da9-8aa6-77f5fdf3a2e2",
   "metadata": {},
   "source": [
    "\n",
    "### **When to use**:\n",
    "- **Classification problems** with **categorical data**.\n",
    "- Determine the relationship between categorical features and a categorical target.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of ANOVA and Chi-Square**:\n",
    "\n",
    "| Technique          | Type of Data            | What it Measures                                  | Use Case                                  |\n",
    "|--------------------|-------------------------|---------------------------------------------------|------------------------------------------|\n",
    "| **ANOVA**          | Continuous Feature, Categorical Target | How different feature groups (values) affect the target | Use for classification (numeric features with categorical target) |\n",
    "| **Chi-Square**     | Categorical Feature, Categorical Target  | How much a feature is related to the target       | Use for classification (categorical features with categorical target) |\n",
    "\n",
    "### **When to use**:\n",
    "- **ANOVA**: When you have a **numeric feature** and a **categorical target**.\n",
    "- **Chi-Square**: When both the **feature** and **target** are **categorical**.\n",
    "\n",
    "Would you like to try these tests with a sample dataset, or do you have specific data you'd like to apply them to?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
