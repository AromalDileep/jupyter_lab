{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8c717c-8eb4-42e0-aabe-6f38304f4e4b",
   "metadata": {},
   "source": [
    "Lasso (**Least Absolute Shrinkage and Selection Operator**) is a popular regression method that applies **L1 regularization** to encourage **sparse models** by shrinking some coefficients to zero. This makes it highly effective for **feature selection** in machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Lasso Works in Feature Selection**\n",
    "1. **L1 Regularization Term**:  \n",
    "   Lasso adds a penalty proportional to the **absolute value of the coefficients** (\\( |w_j| \\)) to the loss function of a linear regression model. The objective function is:\n",
    "\n",
    "   \\[\n",
    "   \\text{Minimize: } \\frac{1}{2n} \\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2 + \\lambda \\sum_{j=1}^p |w_j|\n",
    "   \\]\n",
    "   - \\( y_i \\): Actual target value.\n",
    "   - \\( \\hat{y}_i \\): Predicted value.\n",
    "   - \\( w_j \\): Coefficient for feature \\( j \\).\n",
    "   - \\( \\lambda \\): Regularization strength (controls how much we penalize large coefficients).\n",
    "   - \\( n \\): Number of data points.\n",
    "   - \\( p \\): Number of features.\n",
    "\n",
    "   The **L1 penalty** forces some coefficients to shrink exactly to zero, effectively excluding irrelevant features from the model.\n",
    "\n",
    "2. **Feature Selection Mechanism**:  \n",
    "   - Features with **non-zero coefficients** are retained, meaning they contribute significantly to the prediction.\n",
    "   - Features with **zero coefficients** are discarded, meaning they are irrelevant or redundant.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Lasso Performs Feature Selection**\n",
    "- The L1 regularization term introduces a **constraint** that encourages sparsity in the coefficients. Unlike L2 regularization (used in Ridge regression), which shrinks coefficients uniformly, L1 regularization can push some coefficients to exactly **zero**.\n",
    "- This property allows Lasso to automatically select the most important features while ignoring the rest.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Perform Feature Selection with Lasso**\n",
    "1. **Normalize/Standardize Data**:  \n",
    "   Since Lasso is sensitive to feature scales, normalize the data to ensure all features contribute equally.\n",
    "\n",
    "2. **Fit Lasso Regression**:  \n",
    "   Train a Lasso regression model on your data and adjust the \\( \\lambda \\) parameter to control the degree of regularization.\n",
    "\n",
    "3. **Select Features**:  \n",
    "   Identify the features with non-zero coefficients—they are the selected features.\n",
    "\n",
    "---\n",
    "\n",
    "### **Lasso in Python**\n",
    "Here’s an example using Lasso in `scikit-learn`:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate Sample Data\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
    "\n",
    "# Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply Lasso\n",
    "lasso = Lasso(alpha=0.1)  # alpha is the regularization strength (λ)\n",
    "lasso.fit(X_scaled, y)\n",
    "\n",
    "# Get Selected Features\n",
    "selected_features = lasso.coef_ != 0\n",
    "print(\"Selected Features:\", selected_features)\n",
    "print(\"Lasso Coefficients:\", lasso.coef_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of Using Lasso for Feature Selection**\n",
    "- **Automatic Feature Elimination**: Lasso eliminates unimportant features by setting their coefficients to zero.\n",
    "- **Interpretable Models**: Resulting models are simpler and more interpretable due to reduced feature sets.\n",
    "- **Handles Multicollinearity**: It can handle correlated features by selecting one and discarding the others.\n",
    "\n",
    "---\n",
    "\n",
    "### **Limitations**\n",
    "- **Not Ideal for Highly Correlated Features**: Lasso may arbitrarily select one feature from a group of highly correlated features, ignoring others.\n",
    "- **Requires Hyperparameter Tuning**: The regularization strength (\\( \\lambda \\)) must be tuned to balance sparsity and model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "- Selecting significant predictors in **high-dimensional datasets** (e.g., genomics, finance).\n",
    "- Reducing overfitting by discarding irrelevant features.\n",
    "- Building efficient predictive models with fewer features.\n",
    "\n",
    "Lasso is widely used because of its ability to shrink and select features simultaneously, making it an effective tool in feature engineering and preprocessing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
