{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86cddb02-da3a-4e93-9281-944d0d7b03d7",
   "metadata": {},
   "source": [
    "I'll explain Support Vector Machines (SVMs) and how SVM classifiers work:\n",
    "\n",
    "Support Vector Machines (SVMs) are powerful supervised machine learning algorithms used for classification and regression tasks. Let me break down the key concepts:\n",
    "\n",
    "Core Principle:\n",
    "SVMs aim to find the optimal hyperplane that best separates different classes in a dataset. Imagine you have two classes of data points - the SVM tries to find a line (in 2D) or a hyperplane (in higher dimensions) that creates the maximum margin between these classes.\n",
    "\n",
    "Key Components:\n",
    "1. Support Vectors\n",
    "- These are the data points closest to the decision boundary\n",
    "- They \"support\" or define the optimal hyperplane\n",
    "- They are the most critical points in determining the classifier's position\n",
    "\n",
    "2. Margin\n",
    "- The margin is the distance between the hyperplane and the nearest data point from either class\n",
    "- SVMs try to maximize this margin to create the most robust separation\n",
    "\n",
    "Classification Process:\n",
    "- For linearly separable data, SVM finds a straight line/hyperplane that cleanly separates classes\n",
    "- For non-linearly separable data, SVMs use kernel tricks to transform the data into a higher-dimensional space where linear separation becomes possible\n",
    "\n",
    "Kernel Techniques:\n",
    "SVMs can handle non-linear data through different kernels:\n",
    "- Linear Kernel: Works for linearly separable data\n",
    "- Polynomial Kernel: Captures more complex relationships\n",
    "- Radial Basis Function (RBF) Kernel: Handles complex, non-linear boundaries\n",
    "\n",
    "Mathematical Optimization:\n",
    "SVMs solve an optimization problem to:\n",
    "- Maximize the margin between classes\n",
    "- Minimize classification errors\n",
    "- Balance between margin width and misclassification\n",
    "\n",
    "Here's a simple Python example using scikit-learn to demonstrate SVM classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6f61568-13cf-4f46-ac61-a366a10685d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Adjusted parameters to resolve the error\n",
    "X, y = make_classification(\n",
    "    n_samples=100,      # Total number of samples\n",
    "    n_features=2,       # Number of features\n",
    "    n_informative=2,    # Number of informative features\n",
    "    n_redundant=0,      # Number of redundant features\n",
    "    n_classes=2,        # Number of classes\n",
    "    n_clusters_per_class=1,  # Number of clusters per class\n",
    "    random_state=42     # For reproducibility\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create SVM Classifier\n",
    "clf = svm.SVC(kernel='linear')  \n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Optional: Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe63dd67-508b-4911-bf57-753f8e8dcf5e",
   "metadata": {},
   "source": [
    "Practical Applications:\n",
    "- Image classification\n",
    "- Text categorization\n",
    "- Bioinformatics\n",
    "- Financial market predictions\n",
    "- Handwriting recognition\n",
    "\n",
    "Advantages:\n",
    "- Effective in high-dimensional spaces\n",
    "- Memory efficient\n",
    "- Versatile through different kernel functions\n",
    "\n",
    "Limitations:\n",
    "- Can be slow for large datasets\n",
    "- Sensitive to feature scaling\n",
    "- Difficult to interpret compared to some other models\n",
    "\n",
    "Choosing Parameters:\n",
    "- Kernel type\n",
    "- Regularization parameter (C)\n",
    "- Kernel-specific parameters\n",
    "\n",
    "Would you like me to elaborate on any specific aspect of SVMs or show a more detailed implementation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
