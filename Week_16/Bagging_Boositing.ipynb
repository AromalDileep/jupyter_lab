{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6c7a28-14b1-4469-8110-d177f4c22557",
   "metadata": {},
   "source": [
    "# Ensemble Techniques - Bagging and Bossting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004347f0-bc2d-4276-ba9f-f9e076145b77",
   "metadata": {},
   "source": [
    "Ensemble Techiques : Combine multiple ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690adee-546c-4836-8de5-dc14c3740e10",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "- We have multiple base learners (different or same ML algorithms)\n",
    "- In classification : Majority voting classifier\n",
    "- In regression : Average of the majority\n",
    "- All these base learners are getting trained parallelly not sequentialy\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddea78dd-be24-433d-98b1-e28b31bf357f",
   "metadata": {},
   "source": [
    "### **Algorithms**\n",
    "1. Random Forest\n",
    "Random Forest is the most well-known bagging algorithm. It works by:\n",
    "- Creating multiple decision trees on random subsets of the training data\n",
    "- Using random feature selection for each tree\n",
    "- Aggregating predictions through voting (for classification) or averaging (for regression)\n",
    "- Helps reduce overfitting and improves model stability\n",
    "\n",
    "2. Extra Trees (Extremely Randomized Trees)\n",
    "Similar to Random Forest, but with additional randomization:\n",
    "- Creates trees with random split points\n",
    "- Introduces more randomness in tree construction\n",
    "- Often provides even more variance reduction compared to standard Random Forest\n",
    "\n",
    "3. Bagged Decision Trees\n",
    "- Creates multiple decision trees on bootstrap samples of the training data\n",
    "- Each tree is trained on a different subset of the original dataset\n",
    "- Final prediction is an average or majority vote of individual trees\n",
    "- Helps reduce variance and improve model generalization\n",
    "\n",
    "4. Bagged Neural Networks\n",
    "- Trains multiple neural networks on different bootstrap samples\n",
    "- Aggregates predictions through averaging or voting\n",
    "- Can help reduce neural network model variance\n",
    "- Less common than tree-based bagging methods\n",
    "\n",
    "5. Bagged Support Vector Machines (SVM)\n",
    "- Creates multiple SVMs trained on bootstrap samples\n",
    "- Combines predictions to improve overall model performance\n",
    "- Less frequently used compared to tree-based bagging\n",
    "\n",
    "Key benefits of bagging algorithms:\n",
    "- Reduce model variance\n",
    "- Improve prediction stability\n",
    "- Mitigate overfitting\n",
    "- Work well with high-variance, low-bias models\n",
    "\n",
    "Would you like me to elaborate on any of these algorithms or discuss their implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4b2127-d426-498c-b78e-cf9c677d781c",
   "metadata": {},
   "source": [
    "## Boosting \n",
    "- We have weak learners\n",
    "- All these models are connected sequentialy\n",
    "- When we combine this we get a strong learner\n",
    "  \n",
    "1. Adaboost\n",
    "2. Gradient Boosting\n",
    "3. Extreme Gradient Boost (xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ce3c44-27d9-4d73-91bf-453cc1ee86e8",
   "metadata": {},
   "source": [
    "### **Algorithms**\n",
    "1. AdaBoost (Adaptive Boosting)\n",
    "- One of the first boosting algorithms\n",
    "- Sequentially trains weak learners\n",
    "- Gives more weight to misclassified instances in subsequent iterations\n",
    "- Adjusts model weights based on previous models' performance\n",
    "\n",
    "2. Gradient Boosting Machine (GBM)\n",
    "- Builds trees sequentially, with each tree correcting errors of previous trees\n",
    "- Minimizes loss function using gradient descent\n",
    "- Highly flexible and powerful algorithm\n",
    "- Can be used for both classification and regression\n",
    "\n",
    "3. XGBoost (Extreme Gradient Boosting)\n",
    "- Advanced implementation of gradient boosting\n",
    "- Includes regularization to prevent overfitting\n",
    "- Supports parallel processing\n",
    "- Handles missing values efficiently\n",
    "- Often performs exceptionally well in machine learning competitions\n",
    "\n",
    "4. LightGBM (Light Gradient Boosting Machine)\n",
    "- Developed by Microsoft\n",
    "- Uses leaf-wise tree growth instead of level-wise\n",
    "- More memory-efficient\n",
    "- Faster training speed compared to traditional gradient boosting\n",
    "- Works well with large datasets\n",
    "\n",
    "5. CatBoost\n",
    "- Developed by Yandex\n",
    "- Handles categorical features exceptionally well\n",
    "- Uses symmetric tree growing technique\n",
    "- Reduces prediction shift caused by prediction leakage\n",
    "- Minimizes gradient bias\n",
    "\n",
    "6. Stochastic Gradient Boosting\n",
    "- Introduces randomness in the boosting process\n",
    "- Samples data and features randomly in each iteration\n",
    "- Helps reduce overfitting\n",
    "- Improves model generalization\n",
    "\n",
    "Core principles of boosting algorithms:\n",
    "- Sequentially train weak learners\n",
    "- Focus on correcting previous models' mistakes\n",
    "- Combine multiple models to create a strong predictive model\n",
    "- Reduce bias and improve overall model performance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
