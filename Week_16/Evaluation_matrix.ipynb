{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d9904e2-b998-4a01-b263-aea6f395dea9",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Classification\n",
    "\n",
    "Evaluation metrics for classification are tools used to assess the performance of a classification model. These metrics help in understanding how well the model predicts the target class and whether it meets the desired criteria, such as precision, recall, or overall accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Accuracy\n",
    "**Definition**: The proportion of correct predictions to the total number of predictions.\n",
    "\n",
    "```python\n",
    "# Accuracy Formula\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "```\n",
    "\n",
    "Mathematically represented as:\n",
    "\n",
    "$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "Where:\n",
    "- $TP$: True Positive\n",
    "- $TN$: True Negative\n",
    "- $FP$: False Positive\n",
    "- $FN$: False Negative\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Precision\n",
    "**Definition**: Measures how many of the predicted positive instances are actually positive.\n",
    "\n",
    "```python\n",
    "# Precision Formula\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "```\n",
    "\n",
    "\n",
    "Mathematically represented as:\n",
    "\n",
    "$\\text{Precision} = \\frac{TP}{TP + FP}$\n",
    "\n",
    "Definition: Precision is the proportion of correctly predicted positive instances (true positives) to the total predicted positive instances (true positives + false positives).\n",
    "It measures the accuracy of positive predictions.\n",
    "\n",
    "Intuition: Precision answers the question:\n",
    "\"Of all the instances that were predicted as positive, how many were actually positive?\"\n",
    "\n",
    "Use Case: Precision is important when false positives are costly (e.g., detecting diseases, spam email classification).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Recall (Sensitivity)\n",
    "**Definition**: Measures how many of the actual positive instances were correctly predicted.\n",
    "\n",
    "```python\n",
    "# Recall Formula\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "```\n",
    "\n",
    "Mathematically represented as:\n",
    "\n",
    "$\\text{Recall} = \\frac{TP}{TP + FN}$\n",
    "\n",
    "Definition: Recall is the proportion of correctly predicted positive instances (true positives) to the total actual positive instances (true positives + false negatives).\n",
    "It measures how well the model identifies all the positive cases.\n",
    "\n",
    "Intuition: Recall answers the question:\n",
    "\"Of all the actual positive instances, how many were predicted as positive?\"\n",
    "\n",
    "Use Case: Recall is crucial when false negatives are costly (e.g., medical diagnosis, fraud detection).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. F1 Score\n",
    "**Definition**: Harmonic mean of Precision and Recall, providing a balance between the two metrics.\n",
    "\n",
    "```python\n",
    "# F1 Score Formula\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "```\n",
    "\n",
    "Mathematically represented as:\n",
    "\n",
    "$\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "\n",
    "Definition:\n",
    "The F1 Score provides a single performance metric that considers both false positives and false negatives. It is particularly useful when dealing with imbalanced datasets, where the number of positive and negative instances is significantly different.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Specificity\n",
    "**Definition**: Measures how many of the actual negative instances were correctly identified.\n",
    "\n",
    "```python\n",
    "# Specificity Formula\n",
    "specificity = true_negatives / (true_negatives + false_positives)\n",
    "```\n",
    "\n",
    "Mathematically represented as:\n",
    "\n",
    "$\\text{Specificity} = \\frac{TN}{TN + FP}$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Log Loss\n",
    "**Definition**: Measures the uncertainty of predictions based on how far off they are from the actual class labels.\n",
    "\n",
    "```python\n",
    "# Log Loss Formula\n",
    "import numpy as np\n",
    "\n",
    "def log_loss(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "```\n",
    "\n",
    "Mathematically represented as:\n",
    "\n",
    "$\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i) \\right]$\n",
    "\n",
    "Where:\n",
    "- $y_i$ is the true label\n",
    "- $p_i$ is the predicted probability of the positive class\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Matthews Correlation Coefficient (MCC)\n",
    "**Definition**: Measures the quality of binary classifications considering all confusion matrix categories.\n",
    "\n",
    "```python\n",
    "# MCC Formula\n",
    "mcc = ((true_positives * true_negatives) - (false_positives * false_negatives)) / \\\n",
    "      np.sqrt((true_positives + false_positives) * \n",
    "              (true_positives + false_negatives) * \n",
    "              (true_negatives + false_positives) * \n",
    "              (true_negatives + false_negatives))\n",
    "```\n",
    "\n",
    "Mathematically represented as:\n",
    "\n",
    "$\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$\n",
    "\n",
    "---\n",
    "\n",
    "## Choosing the Right Metric\n",
    "- **Balanced Classes**: Use **accuracy**\n",
    "- **Imbalanced Classes**: Use **precision**, **recall**, **F1 score**, **AUC-ROC**\n",
    "- **Probabilistic Models**: Use **log loss** or **AUC-ROC**\n",
    "\n",
    "**Tip**: Always consider the specific context of your problem when selecting an evaluation metric!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
