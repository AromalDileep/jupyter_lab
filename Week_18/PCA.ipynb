{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52706f67-791b-4717-b6a4-25d1b0319184",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) Step-by-Step\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform a dataset into a new coordinate system such that the greatest variances are along the first axes (called principal components).\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Standardize the Dataset\n",
    "Since PCA is sensitive to the scale of features, standardize the dataset so that each feature has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "The formula for standardization is:  \n",
    "$z = \\frac{x - \\mu}{\\sigma}$  \n",
    "\n",
    "Where:  \n",
    "- $x$ is the original feature value,  \n",
    "- $\\mu$ is the mean of the feature,  \n",
    "- $\\sigma$ is the standard deviation of the feature.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Compute the Covariance Matrix\n",
    "The covariance matrix measures how features co-vary, i.e., how changes in one feature correspond to changes in another.\n",
    "\n",
    "The covariance matrix is computed as:  \n",
    "$\\text{Cov}(X) = \\frac{1}{n-1} \\cdot X^T X$  \n",
    "\n",
    "Where:  \n",
    "- $X$ is the standardized data matrix (rows are samples, columns are features),  \n",
    "- $X^T$ is the transpose of $X$.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Calculate the Eigenvalues and Eigenvectors\n",
    "- Compute the **eigenvalues** and **eigenvectors** of the covariance matrix.  \n",
    "- Eigenvalues represent the amount of variance captured by each principal component.  \n",
    "- Eigenvectors represent the directions (principal axes) in the feature space.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Sort the Eigenvalues and Eigenvectors\n",
    "- Sort the eigenvalues in descending order.  \n",
    "- Rearrange the eigenvectors to correspond to the sorted eigenvalues.  \n",
    "- The eigenvector corresponding to the largest eigenvalue is the **first principal component**, which explains the maximum variance.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Select the Top $k$ Principal Components\n",
    "Decide how many principal components ($k$) to retain. This is based on the **explained variance ratio**:  \n",
    "$\\text{Explained Variance Ratio} = \\frac{\\text{Eigenvalue}_i}{\\sum \\text{Eigenvalues}}$  \n",
    "\n",
    "Choose $k$ such that the cumulative explained variance meets a desired threshold (e.g., 95%).\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Project the Data onto the New Basis\n",
    "Transform the original dataset to the new coordinate system defined by the top $k$ principal components. The transformation formula is:  \n",
    "$Z = X \\cdot W$  \n",
    "\n",
    "Where:  \n",
    "- $Z$ is the transformed dataset in the reduced dimensionality space,  \n",
    "- $X$ is the standardized data,  \n",
    "- $W$ is the matrix of selected eigenvectors (corresponding to the top $k$ eigenvalues).\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7: Interpretation\n",
    "- The transformed dataset $Z$ has reduced dimensions.  \n",
    "- Each axis corresponds to a principal component, and each component captures a portion of the variance in the original data.\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization of PCA Steps:\n",
    "\n",
    "If your data is 2D or 3D, PCA can be visualized as rotating the original axes to align with the directions of maximum variance and then projecting the data onto these new axes.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
