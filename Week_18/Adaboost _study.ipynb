{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lylmlnaZe2UF"
   },
   "source": [
    "#  step-by-step explanation of how AdaBoost works, explained in a simple way:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Initialize Sample Weights**\n",
    "- Start with assigning equal weights to all samples in the dataset.\n",
    "  - If there are $ N $ samples, each sample gets a weight of $ \\frac{1}{N} $.\n",
    "  - These weights represent how \"important\" each sample is during training.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Train the First Weak Learner**\n",
    "- Train a simple model (like a decision stump) on the dataset.\n",
    "- The model tries to classify the samples.\n",
    "- Calculate the **error rate** ($ e_t $) of the model:\n",
    "\n",
    "  $ e_t = \\frac{\\text{Total weight of misclassified samples}}{\\text{Total weight of all samples}} $\n",
    "\n",
    "  - This gives more weight to samples that were harder to classify.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Calculate the Model's \"Say\" ($ \\alpha_t $)**\n",
    "(Perfomance of the stump)\n",
    "- The better the model, the more \"say\" it gets in making predictions.\n",
    "- Compute:\n",
    "\n",
    "  $ \\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - e_t}{e_t}\\right) $\n",
    "\n",
    "  - $ \\alpha_t > 0 $: Good performance.\n",
    "\n",
    "  - $ \\alpha_t < 0 $: Poor performance (this rarely happens, as weak learners are expected to perform slightly better than random guessing).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Update Weights of the Samples**\n",
    "- Update the weights to focus more on misclassified samples.\n",
    "- For **correctly classified samples**, reduce their weight:\n",
    "\n",
    "  $ w_{i}^{(t+1)} = w_{i}^{(t)} \\cdot e^{-\\alpha_t} $\n",
    "\n",
    "- For **misclassified samples**, increase their weight:\n",
    "\n",
    "  $ w_{i}^{(t+1)} = w_{i}^{(t)} \\cdot e^{\\alpha_t} $\n",
    "\n",
    "- Normalize the weights so they sum to 1:\n",
    "\n",
    "  $ w_{i}^{(t+1)} = \\frac{w_{i}^{(t+1)}}{\\sum_{j=1}^N w_{j}^{(t+1)}} $\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Repeat for Multiple Weak Learners**\n",
    "- Train another weak learner on the updated weights.\n",
    "- Repeat steps 2–4 for $ T $ iterations, where $ T $ is the number of weak learners.\n",
    "- Each weak learner focuses more on the samples that the previous learners struggled with.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Combine the Weak Learners**\n",
    "- At the end, combine all the weak learners into a single strong model.\n",
    "- The prediction of the strong model is a weighted sum of the predictions of the weak learners:\n",
    "\n",
    "  $ H(x) = \\text{sign}\\left(\\sum_{t=1}^T \\alpha_t \\cdot h_t(x)\\right) $\n",
    "\n",
    "  - $ \\alpha_t $: Weight (or \"say\") of the $ t $-th weak learner.\n",
    "\n",
    "  - $ h_t(x) $: Prediction of the $ t $-th weak learner.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Ideas**\n",
    "- **Focus on Hard Cases**: AdaBoost shifts focus to samples that are hard to classify.\n",
    "- **Combine Weak Learners**: Weak learners (e.g., decision stumps) are combined to form a strong, accurate classifier.\n",
    "- **Weighted Votes**: Learners that perform better get more influence on the final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example (Intuition)**\n",
    "Imagine you're teaching a group of students for a test:\n",
    "1. In the first session, you focus equally on all students, but some students fail to understand.\n",
    "2. In the second session, you spend more time helping the struggling students.\n",
    "3. In the third session, you again adjust your focus based on who still needs help.\n",
    "4. At the end, you combine all your efforts, and everyone is better prepared.\n",
    "\n",
    "Similarly, AdaBoost keeps adjusting its focus on the \"hard-to-learn\" samples and builds a strong classifier by combining weak ones.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlCmXSLQ3WBN"
   },
   "source": [
    "# Updating the weights in Adaboost for ***correctly*** classified samples\n",
    "In AdaBoost, the weights of samples are updated after each weak learner (stump) is trained. For **correctly classified samples**, the weight is decreased to give less focus on those samples in the next iteration. The weight update formula for correctly classified samples is as follows:\n",
    "\n",
    "### Formula:\n",
    "\n",
    "$\n",
    "w_{i}^{(t+1)} = w_{i}^{(t)} \\cdot e^{-\\alpha_t}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ w_{i}^{(t+1)} $: Weight of the $i$-th sample for the next iteration.\n",
    "- $ w_{i}^{(t)}$: Weight of the $i$-th sample in the current iteration.\n",
    "- $ \\alpha_t $: The amount of say (or weight) of the current weak learner $t$, calculated as:\n",
    "  \n",
    "  $\n",
    "  \\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - e_t}{e_t}\\right)\n",
    "  $\n",
    "\n",
    "  where $ e_t \\$ is the weighted error of the weak learner:\n",
    "  \n",
    "  $\n",
    "  e_t = \\frac{\\sum_{i=1}^N w_i^{(t)} \\cdot I(y_i \\neq h_t(x_i))}{\\sum_{i=1}^N w_i^{(t)}}\n",
    "  $\n",
    "  - $ I(y_i \\neq h_t(x_i)) $: Indicator function, equal to 1 if the weak learner misclassifies the sample, otherwise 0.\n",
    "\n",
    "For **normalization** of weights after updates:\n",
    "\n",
    "$\n",
    "w_{i}^{(t+1)} \\leftarrow \\frac{w_{i}^{(t+1)}}{\\sum_{j=1}^N w_{j}^{(t+1)}}\n",
    "$\n",
    "\n",
    "This ensures that the total weights sum to 1.\n",
    "\n",
    "### Intuition:\n",
    "1. If a sample is correctly classified, the factor $ e^{-\\alpha_t} $ decreases its weight.\n",
    "2. As the weak learner gets better (i.e., lower error $e_t$), $ \\alpha_t $ increases, leading to a greater decrease in weight for correctly classified samples.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djMF1cpHWkuN"
   },
   "source": [
    "# Updating the weights in Adaboost for ***incorrectly*** classified samples\n",
    "\n",
    "---\n",
    "\n",
    "In **AdaBoost**, the weights for incorrectly classified samples are updated to give them higher importance in the next iteration. Here's the formula used for updating weights for incorrectly classified samples:\n",
    "\n",
    "### Formula:\n",
    "\n",
    "$\n",
    "w_{i}^{(t+1)} = w_{i}^{(t)} \\cdot e^{\\alpha_t}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $w_{i}^{(t+1)}$: Weight of the $i^{\\text{th}}$ sample at the $t+1$-th iteration.\n",
    "- $w_{i}^{(t)}$: Weight of the $i^{\\text{th}}$ sample at the $t$-th iteration.\n",
    "- $\\alpha_t$: Weight of the weak learner (logarithmic measure of its accuracy), given by:\n",
    "  $\n",
    "  \\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - e_t}{e_t}\\right)\n",
    "  $\n",
    "  - $e_t$: Error rate of the weak learner at iteration $t$.\n",
    "\n",
    "### Steps for Weight Update:\n",
    "1. **If the sample is ***incorrectly*** classified:**\n",
    "   $\n",
    "   w_{i}^{(t+1)} = w_{i}^{(t)} \\cdot e^{\\alpha_t}\n",
    "   $\n",
    "   This increases the weight of the sample so that it gets more attention in the next round.\n",
    "\n",
    "2. **If the sample is correctly classified:**\n",
    "   $\n",
    "   w_{i}^{(t+1)} = w_{i}^{(t)} \\cdot e^{-\\alpha_t}\n",
    "   $\n",
    "   This decreases the weight of the sample since it is already classified correctly.\n",
    "\n",
    "3. **Normalization:** After updating the weights, normalize them to ensure they sum to 1:\n",
    "   $\n",
    "   w_{i}^{(t+1)} \\gets \\frac{w_{i}^{(t+1)}}{\\sum_{j=1}^n w_{j}^{(t+1)}}\n",
    "   $\n",
    "\n",
    "This ensures that the algorithm focuses more on difficult samples in subsequent iterations.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn4YitknqWlJ"
   },
   "source": [
    "# The $e$ in the formula\n",
    "The $ e $  in the first formula $ w_{i}^{(t+1)} = w_{i}^{(t)} \\cdot e^{-\\alpha_t} $ refers to the **base of the natural logarithm** (Euler's number), which is approximately equal to $ 2.718 $.\n",
    "\n",
    "### Why is $ e $ used in AdaBoost?\n",
    "\n",
    "1. **Exponential weight adjustment**: The AdaBoost algorithm uses an exponential function $ e^{-\\alpha_t} $ to decrease the weights of correctly classified samples. This ensures that the weight adjustment is proportional to the confidence (or \"amount of say\") of the weak learner, $ \\alpha_t $.\n",
    "\n",
    "2. **Mathematical convenience**: Exponential functions and logarithms (which are their inverses) are widely used in machine learning due to their smoothness and the properties that simplify mathematical operations like derivatives and scaling.\n",
    "\n",
    "In essence, $ e $ is a mathematical constant used to create the exponential scaling factor for the weights in the AdaBoost algorithm.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXNgmVnLqq83"
   },
   "source": [
    "---\n",
    "---\n",
    "### AdaBoost (Adaptive Boosting) Formula and Key Concepts\n",
    "\n",
    "1. **AdaBoost Algorithm Overview**:\n",
    "   AdaBoost is an ensemble learning method that combines multiple weak learners (usually decision trees) to create a strong learner. It works by sequentially training models, each focusing on the mistakes of the previous model.\n",
    "\n",
    "2. **AdaBoost Weight Update Formula**:\n",
    "\n",
    "   The prediction in AdaBoost is based on a weighted sum of the predictions from each weak learner. Each weak learner $ h_t(x) $ is assigned a weight $ \\alpha_t $ based on its performance.\n",
    "\n",
    "   - **Model Prediction**:\n",
    "     $$ H(x) = \\text{sign}\\left( \\sum_{t=1}^T \\alpha_t h_t(x) \\right) $$\n",
    "\n",
    "   - **Explanation**:\n",
    "     - $ H(x) $: Final prediction of the ensemble.\n",
    "     - $ h_t(x) $: Prediction of the weak learner $ t $.\n",
    "     - $ \\alpha_t $: Weight of weak learner $ t $, based on its accuracy.\n",
    "\n",
    "3. **Calculation of Learner Weight ($ \\alpha_t $)**:\n",
    "   The weight $ \\alpha_t $ is calculated based on the error rate of the weak learner:\n",
    "\n",
    "   $$ \\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right) $$\n",
    "\n",
    "   - **Explanation**:\n",
    "     - $ \\epsilon_t $: The error rate of the weak learner $ h_t $, i.e., the weighted misclassification error.\n",
    "\n",
    "4. **Update Sample Weights**:\n",
    "   After each round of training, the weights of the misclassified samples are increased, so the next model focuses more on these samples. The weight update formula is:\n",
    "\n",
    "   $$ D_{t+1}(i) = D_t(i) \\cdot \\exp\\left( -\\alpha_t y_i h_t(x_i) \\right) $$\n",
    "\n",
    "   - **Explanation**:\n",
    "     - $ D_t(i) $: Weight of sample $ i $ at round $ t $.\n",
    "     - $ y_i $: True label of sample $ i $.\n",
    "     - $ h_t(x_i) $: Prediction of the weak learner for sample $ i $.\n",
    "     - $ \\alpha_t $: Weight of the weak learner.\n",
    "\n",
    "   The sample weights are then normalized to ensure they sum to 1.\n",
    "\n",
    "5. **Boosting Process**:\n",
    "   - **Initialize weights**: All training samples are given equal weight initially.\n",
    "   - **Train weak learner**: Train a weak learner on the weighted data, using the current weights.\n",
    "   - **Calculate learner error**: Compute the error rate $ \\epsilon_t $ and update the learner’s weight $ \\alpha_t $.\n",
    "   - **Update sample weights**: Increase the weights of misclassified samples and decrease the weights of correctly classified samples.\n",
    "   - **Repeat** for a specified number of rounds $ T $.\n",
    "\n",
    "### Key Parameters:\n",
    "- **$ T $**: Number of weak learners (iterations).\n",
    "- **$ \\alpha_t $**: Weight of the weak learner $ t $.\n",
    "- **$ D_t(i) $**: Weight of sample $ i $ at round $ t $.\n",
    "- **$ \\epsilon_t $**: Error rate of the weak learner $ t $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOf9KJexR/LpAV7KxioxXTi",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
