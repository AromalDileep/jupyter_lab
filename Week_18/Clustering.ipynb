{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63dff10-b064-4e18-9a5f-dd3517c96f17",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **1. What is Clustering?**\n",
    "\n",
    "### **Definition**\n",
    "Clustering is an **unsupervised machine learning technique** used to group data points into clusters, where:\n",
    "- **Data points in the same cluster** are more similar to each other.\n",
    "- **Data points in different clusters** are as distinct as possible.\n",
    "\n",
    "### **Applications of Clustering**\n",
    "- Market segmentation (grouping customers based on behavior or demographics).\n",
    "- Image segmentation (e.g., dividing an image into regions).\n",
    "- Anomaly detection (e.g., identifying fraudulent transactions).\n",
    "- Document clustering (e.g., grouping similar news articles).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Clustering Concepts**\n",
    "\n",
    "### **Similarity/Dissimilarity**\n",
    "- **Similarity** measures how close two data points are. (e.g., Euclidean distance, cosine similarity).\n",
    "- **Dissimilarity** measures how far apart two data points are.\n",
    "\n",
    "### **Distance Metrics** \n",
    "Some commonly used metrics:\n",
    "- **Euclidean Distance:** $ \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2} $\n",
    "- **Manhattan Distance:** $ \\sum_{i=1}^{n} |x_i - y_i| $\n",
    "- **Cosine Similarity:** Measures the angle between vectors (used in high-dimensional data).\n",
    "\n",
    "### **Important Terminology**\n",
    "- **Centroid:** The center of a cluster.\n",
    "- **Cluster Density:** How closely packed the points in a cluster are.\n",
    "- **Intra-cluster Distance:** Distance between points in the same cluster (should be low).\n",
    "- **Inter-cluster Distance:** Distance between points in different clusters (should be high).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Types of Clustering**\n",
    "\n",
    "### **A. Partition-Based Clustering**\n",
    "- Divides data into non-overlapping clusters.\n",
    "- **Example Algorithm:** K-Means\n",
    "\n",
    "#### **K-Means Clustering**\n",
    "1. Choose $k$: the number of clusters.\n",
    "2. Randomly initialize $ k $ centroids.\n",
    "3. Assign each point to the nearest centroid.\n",
    "4. Recompute centroids based on cluster assignments.\n",
    "5. Repeat steps 3â€“4 until centroids stabilize.\n",
    "\n",
    "**Use Case:** Grouping customers in a retail store based on purchase patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### **B. Hierarchical Clustering**\n",
    "- Builds a hierarchy of clusters (tree-like structure called a dendrogram).\n",
    "- Two main types:\n",
    "  1. **Agglomerative (Bottom-Up):** Start with each point as its own cluster, then merge.\n",
    "  2. **Divisive (Top-Down):** Start with all points in one cluster, then split.\n",
    "\n",
    "#### **Steps in Agglomerative Clustering:**\n",
    "1. Compute a distance matrix for all points.\n",
    "2. Merge the two closest clusters.\n",
    "3. Recompute the distance matrix.\n",
    "4. Repeat until a single cluster remains.\n",
    "\n",
    "**Use Case:** Analyzing evolutionary relationships in biology.\n",
    "\n",
    "---\n",
    "\n",
    "### **C. Density-Based Clustering**\n",
    "- Groups data points based on density.\n",
    "- Detects clusters of arbitrary shapes and handles noise well.\n",
    "- **Example Algorithm:** DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "#### **DBSCAN Key Ideas:**\n",
    "1. Define two parameters:\n",
    "   - $ \\varepsilon $ (Epsilon): Radius of a neighborhood.\n",
    "   - MinPts: Minimum number of points required to form a dense region.\n",
    "2. Points are classified as:\n",
    "   - **Core Point:** Enough points in its neighborhood.\n",
    "   - **Border Point:** Near a core point but not dense itself.\n",
    "   - **Noise Point:** Neither core nor border.\n",
    "\n",
    "**Use Case:** Identifying anomalies in large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **D. Model-Based Clustering**\n",
    "- Assumes data is generated by a mixture of underlying statistical distributions.\n",
    "- Tries to fit a probabilistic model to the data.\n",
    "- **Example Algorithm:** Gaussian Mixture Model (GMM)\n",
    "\n",
    "#### **Gaussian Mixture Model**\n",
    "1. Assumes data points are generated from a mixture of Gaussian distributions.\n",
    "2. Uses **Expectation-Maximization (EM)** to estimate parameters like means and variances of Gaussians.\n",
    "\n",
    "**Use Case:** Image compression (e.g., grouping similar pixel colors).\n",
    "\n",
    "---\n",
    "\n",
    "### **E. Grid-Based Clustering**\n",
    "- Divides the space into a finite number of cells and clusters based on density in those cells.\n",
    "- **Example Algorithm:** STING (Statistical Information Grid-based Clustering)\n",
    "\n",
    "**Use Case:** Spatial data analysis, such as clustering geographical locations.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Choosing the Right Clustering Technique**\n",
    "\n",
    "| **Criteria**               | **Recommended Algorithm**     |\n",
    "|-----------------------------|-------------------------------|\n",
    "| Clusters with spherical shapes | K-Means                     |\n",
    "| Hierarchical relationships   | Agglomerative or Divisive    |\n",
    "| Arbitrary-shaped clusters    | DBSCAN                       |\n",
    "| Probabilistic modeling       | Gaussian Mixture Model (GMM) |\n",
    "| High-dimensional data        | Spectral Clustering          |\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Evaluation Metrics for Clustering**\n",
    "\n",
    "Since clustering is unsupervised, evaluation is based on intrinsic properties:\n",
    "- **Silhouette Score:** Measures how similar a point is to its own cluster vs other clusters.\n",
    "  - Range: $-1$ (bad clustering) to $1$ (good clustering).\n",
    "- **Davies-Bouldin Index:** Measures the average similarity ratio of clusters. Lower is better.\n",
    "- **Dunn Index:** Measures the ratio of the smallest distance between clusters to the largest intra-cluster distance. Higher is better.\n",
    "\n",
    "---\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff4813-eca7-487a-83a6-287a715ed3ef",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Clustering Concepts\n",
    "\n",
    "Clustering is an **unsupervised machine learning technique** used to group similar data points into clusters based on their features. The goal is to identify hidden patterns or structures in data without predefined labels. Each cluster represents a collection of data points that are more similar to each other than to those in other clusters.\n",
    "\n",
    "#### Key Concepts:\n",
    "1. **Cluster**: A group of data points with high similarity within the group and low similarity to other groups.\n",
    "2. **Similarity/Dissimilarity**: \n",
    "   - Similarity is often measured using metrics like **Euclidean distance**, **Cosine similarity**, or **Manhattan distance**.\n",
    "   - Dissimilarity is the opposite of similarity.\n",
    "3. **Centroid**: The center point of a cluster, often used in algorithms like K-Means.\n",
    "4. **Inertia**: Measures how tightly data points are grouped within clusters.\n",
    "5. **Cluster Validity Metrics**:\n",
    "   - **Silhouette Score**: Measures how similar an object is to its cluster compared to others.\n",
    "   - **Dunn Index**: Balances inter-cluster separation and intra-cluster compactness.\n",
    "6. **Applications**:\n",
    "   - Customer segmentation\n",
    "   - Image segmentation\n",
    "   - Document clustering\n",
    "   - Anomaly detection\n",
    "\n",
    "---\n",
    "\n",
    "### Clustering Types\n",
    "\n",
    "1. **Partition-based Clustering**:\n",
    "   - Divides the dataset into non-overlapping clusters.\n",
    "   - **Algorithms**:\n",
    "     - **K-Means**: Iteratively assigns data points to the nearest cluster centroid.\n",
    "     - **K-Medoids**: Uses actual data points as cluster centers (medoids).\n",
    "   - **Pros**:\n",
    "     - Simple and fast.\n",
    "   - **Cons**:\n",
    "     - Sensitive to initial centroid placement.\n",
    "     - Requires specifying the number of clusters beforehand.\n",
    "\n",
    "2. **Hierarchical Clustering**:\n",
    "   - Builds a tree-like structure of clusters.\n",
    "   - **Types**:\n",
    "     - **Agglomerative**: Starts with individual points as clusters and merges them iteratively.\n",
    "     - **Divisive**: Starts with all points in one cluster and divides them iteratively.\n",
    "   - **Pros**:\n",
    "     - No need to predefine the number of clusters.\n",
    "   - **Cons**:\n",
    "     - Computationally expensive for large datasets.\n",
    "\n",
    "3. **Density-based Clustering**:\n",
    "   - Forms clusters based on dense regions of data points.\n",
    "   - **Algorithms**:\n",
    "     - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: Groups points that are closely packed together and labels points in low-density regions as noise.\n",
    "     - **OPTICS (Ordering Points To Identify Clustering Structure)**: Extends DBSCAN for varying densities.\n",
    "   - **Pros**:\n",
    "     - Can handle noise and irregularly shaped clusters.\n",
    "   - **Cons**:\n",
    "     - Struggles with clusters of varying densities.\n",
    "\n",
    "4. **Model-based Clustering**:\n",
    "   - Assumes the data is generated by a mixture of probability distributions.\n",
    "   - **Algorithms**:\n",
    "     - **Gaussian Mixture Models (GMMs)**: Assumes each cluster follows a Gaussian distribution.\n",
    "   - **Pros**:\n",
    "     - Provides probabilistic cluster assignments.\n",
    "   - **Cons**:\n",
    "     - Computationally expensive.\n",
    "\n",
    "5. **Grid-based Clustering**:\n",
    "   - Divides the data space into a grid structure and performs clustering on the grids.\n",
    "   - **Algorithms**:\n",
    "     - **CLIQUE (Clustering In Quest)**: Combines density and grid-based clustering.\n",
    "   - **Pros**:\n",
    "     - Efficient for high-dimensional data.\n",
    "   - **Cons**:\n",
    "     - Requires careful grid size selection.\n",
    "\n",
    "6. **Spectral Clustering**:\n",
    "   - Uses eigenvalues of similarity matrices to perform dimensionality reduction before clustering.\n",
    "   - **Pros**:\n",
    "     - Works well with non-convex clusters.\n",
    "   - **Cons**:\n",
    "     - Computationally expensive for large datasets.\n",
    "\n",
    "7. **Fuzzy Clustering**:\n",
    "   - Assigns each data point a membership score for each cluster.\n",
    "   - **Algorithm**:\n",
    "     - **Fuzzy C-Means (FCM)**: Similar to K-Means but allows soft clustering.\n",
    "   - **Pros**:\n",
    "     - Handles overlapping clusters.\n",
    "   - **Cons**:\n",
    "     - Sensitive to initial conditions.\n",
    "\n",
    "8. **Constraint-based Clustering**:\n",
    "   - Incorporates domain-specific constraints while clustering.\n",
    "   - **Pros**:\n",
    "     - Allows for custom clustering based on business rules.\n",
    "   - **Cons**:\n",
    "     - Complex to implement.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307e77e1-2e46-476e-b5d4-4fa8d90ab8d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "Clustering evaluation methods assess the performance of clustering algorithms. These methods are generally categorized into **internal evaluation**, **external evaluation**, and **relative evaluation**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Internal Evaluation Methods**\n",
    "These methods use information from the data itself to measure the quality of clustering without any ground truth labels.\n",
    "\n",
    "#### (a) **Silhouette Coefficient**\n",
    "- Measures the compactness and separation of clusters.\n",
    "- Formula:\n",
    "  \n",
    "  $$ S(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))} $$\n",
    "  \n",
    "  - $a(i)$: Average intra-cluster distance (within the same cluster).\n",
    "  - $b(i)$: Average inter-cluster distance (to the nearest other cluster).\n",
    "- Range: $[-1, 1]$. Higher values indicate better clustering.\n",
    "\n",
    "#### (b) **Davies-Bouldin Index (DBI)**\n",
    "- Evaluates intra-cluster similarity and inter-cluster separation.\n",
    "- Formula:\n",
    "\n",
    "  $$ DBI = \\frac{1}{N} \\sum_{i=1}^{N} \\max_{i \\neq j} \\left( \\frac{s_i + s_j}{d_{i,j}} \\right) $$\n",
    "  \n",
    "  - $s_i$: Average distance of points in cluster $i$ to the cluster center.\n",
    "  - $d_{i,j}$: Distance between cluster centers $i$ and $j$.\n",
    "- Lower values indicate better clustering.\n",
    "\n",
    "#### (c) **Dunn Index**\n",
    "- Measures the ratio of the minimum inter-cluster distance to the maximum intra-cluster distance.\n",
    "- Formula:\n",
    "  \n",
    "  $$ \\text{Dunn} = \\frac{\\min_{i \\neq j} d(c_i, c_j)}{\\max_k \\delta(c_k)} $$\n",
    "  \n",
    "  - $d(c_i, c_j)$: Distance between cluster centers $i$ and $j$.\n",
    "  - $\\delta(c_k)$: Diameter of cluster $k$.\n",
    "- Higher values indicate better clustering.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **External Evaluation Methods**\n",
    "These methods compare the clustering result to ground truth labels, if available.\n",
    "\n",
    "#### (a) **Adjusted Rand Index (ARI)**\n",
    "- Measures the similarity between the clustering result and ground truth, adjusted for chance.\n",
    "- Formula:\n",
    "\n",
    "  $$ ARI = \\frac{\\text{Index} - \\text{Expected Index}}{\\text{Max Index} - \\text{Expected Index}} $$\n",
    "\n",
    "- Range: $[-1, 1]$; higher values indicate better clustering.\n",
    "\n",
    "#### (b) **Normalized Mutual Information (NMI)**\n",
    "- Measures the amount of information shared between the clustering result and ground truth.\n",
    "- Formula:\n",
    "\n",
    "  $$ NMI = \\frac{2 \\cdot I(U, V)}{H(U) + H(V)} $$\n",
    "\n",
    "  - $I(U, V)$: Mutual information between the true labels ($U$) and cluster labels ($V$).\n",
    "  - $H(U)$, $H(V)$: Entropies of $U$ and $V$.\n",
    "- Range: $[0, 1]$; higher values indicate better clustering.\n",
    "\n",
    "#### (c) **Fowlkes-Mallows Index (FMI)**\n",
    "- Measures the geometric mean of precision and recall for clustering.\n",
    "- Formula:\n",
    "\n",
    "  $$ FMI = \\sqrt{\\frac{TP}{TP + FP} \\cdot \\frac{TP}{TP + FN}} $$\n",
    "\n",
    "  - $TP$: True Positives.\n",
    "  - $FP$: False Positives.\n",
    "  - $FN$: False Negatives.\n",
    "- Range: $[0, 1]$; higher values indicate better clustering.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Relative Evaluation Methods**\n",
    "These methods compare clustering results with varying parameters or algorithms to determine the best configuration.\n",
    "\n",
    "#### (a) **Elbow Method**\n",
    "- Plots the sum of squared distances (SSD) from points to their cluster centers for different numbers of clusters ($k$).\n",
    "- The \"elbow point\" indicates the optimal $k$.\n",
    "\n",
    "#### (b) **Gap Statistic**\n",
    "- Compares clustering performance against a null reference distribution to find the optimal number of clusters.\n",
    "\n",
    "#### (c) **Silhouette Analysis**\n",
    "- Analyzes the Silhouette Coefficient for different numbers of clusters to find the optimal $k$.\n",
    "\n",
    "---\n",
    "\n",
    "### Choosing an Evaluation Method\n",
    "- Use **internal metrics** if no ground truth labels are available.\n",
    "- Use **external metrics** if ground truth labels are known.\n",
    "- Use **relative metrics** to tune parameters like the number of clusters.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
