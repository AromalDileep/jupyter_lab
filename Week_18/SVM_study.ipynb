{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636acdaf-3721-4fbc-8d2f-b05647ffb4a1",
   "metadata": {},
   "source": [
    "# Support Vector Regression (SVR) in simple terms, step by step, with all the essential concepts \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Problem Statement in Regression**\n",
    "In regression, we want to predict a continuous value $y$ for a given input $x$. For example, predicting house prices based on features like size or location.\n",
    "\n",
    "**Goal of SVR:**\n",
    "Find a function $f(x)$ that:\n",
    "- Predicts the output $y$ with minimal error.\n",
    "- Ensures the predictions are as simple as possible (minimizing complexity).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Core Idea of SVR**\n",
    "Instead of focusing on minimizing the prediction error at every single data point, SVR introduces a \"margin of tolerance\" (called **$\\epsilon$**):\n",
    "- If the prediction is **within $\\epsilon$** of the true value $y$, we **don’t care about the error**.\n",
    "- We only care about points that are **outside $\\epsilon$**. These points are called **Support Vectors**.\n",
    "\n",
    "This is different from ordinary regression, which tries to minimize the error for every data point.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Key Components of SVR**\n",
    "\n",
    "### a) **Loss Function ($\\epsilon$-Insensitive Loss)**\n",
    "The loss function in SVR ignores errors within the $\\epsilon$-margin. The formula for the loss is:\n",
    "\n",
    "$$\n",
    "L(y, f(x)) =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } |y - f(x)| \\leq \\epsilon \\\\\n",
    "|y - f(x)| - \\epsilon & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- No penalty if the prediction is within $\\epsilon$.\n",
    "- Penalty increases linearly for points outside $\\epsilon$.\n",
    "\n",
    "---\n",
    "\n",
    "### b) **Support Vectors**\n",
    "Support Vectors are the data points **outside the $\\epsilon$-margin** or on the boundary of the margin. These points determine the position and shape of the regression line.\n",
    "\n",
    "- **Why are they important?**\n",
    "  SVR doesn’t use all data points. Only the support vectors influence the model. Points inside the $\\epsilon$-margin are ignored.\n",
    "\n",
    "---\n",
    "\n",
    "### c) **Optimization Objective**\n",
    "The goal of SVR is to:\n",
    "1. Minimize the complexity of the model (make the regression function as \"flat\" as possible).\n",
    "   - This is achieved by minimizing $\\frac{1}{2} ||w||^2$, where $w$ represents the weights of the function.\n",
    "2. Ensure all predictions fall within the $\\epsilon$-margin, as much as possible.\n",
    "\n",
    "Formally, the objective is:\n",
    "$$\n",
    "\\text{Minimize: } \\frac{1}{2} ||w||^2 + C \\sum (\\xi + \\xi^*)\n",
    "$$\n",
    "Where:\n",
    "- $||w||^2$: Represents the flatness of the function.\n",
    "- $\\xi, \\xi^*$: Slack variables that handle violations of the $\\epsilon$-margin (for points outside the margin).\n",
    "- $C$: Regularization parameter controlling the trade-off between flatness and margin violations.\n",
    "\n",
    "---\n",
    "\n",
    "### d) **Kernel Trick for Non-Linearity**\n",
    "SVR can handle non-linear data using the **kernel trick**. Instead of fitting a straight line, SVR transforms the input space into a higher-dimensional space (using a kernel function like RBF or polynomial) and fits a hyperplane there.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Step-by-Step Workflow**\n",
    "\n",
    "### Step 1: Define the Margin\n",
    "- Define the $\\epsilon$-margin around the regression line where predictions are considered acceptable.\n",
    "- No penalty for points within the margin.\n",
    "\n",
    "### Step 2: Identify Support Vectors\n",
    "- Support vectors are the points that:\n",
    "  - Lie on the $\\epsilon$-margin boundary.\n",
    "  - Lie outside the $\\epsilon$-margin.\n",
    "\n",
    "### Step 3: Optimize the Function\n",
    "- Minimize the complexity of the regression function (minimize $||w||^2$).\n",
    "- Introduce slack variables ($\\xi, \\xi^*$) to handle points outside the margin.\n",
    "- Balance the trade-off between flatness and errors using the $C$ parameter.\n",
    "\n",
    "### Step 4: Use Kernel Functions (Optional)\n",
    "- If the data is not linearly separable, use kernels to map the data into a higher-dimensional space.\n",
    "- Common kernels: Linear, Polynomial, Radial Basis Function (RBF).\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Important Terms**\n",
    "\n",
    "### a) **Hyperparameters**\n",
    "- $\\epsilon$: Width of the margin. Larger $\\epsilon$ means more tolerance for error.\n",
    "- $C$: Regularization parameter. High $C$ forces the model to fit the data more closely (less tolerance for errors), while low $C$ allows a simpler model with more margin violations.\n",
    "\n",
    "### b) **Dual Problem**\n",
    "The optimization problem in SVR is solved using the **dual formulation** (Lagrange multipliers). This allows the kernel trick to be applied efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. How is SVR Achieved?**\n",
    "\n",
    "1. **Formulate the Problem:**\n",
    "   - Define the $\\epsilon$-margin.\n",
    "   - Write the optimization problem to minimize $||w||^2$ while handling slack variables for violations.\n",
    "\n",
    "2. **Solve Using Quadratic Programming:**\n",
    "   - Use Lagrange multipliers to convert the problem into a dual form.\n",
    "   - Efficiently solve the problem using optimization techniques.\n",
    "\n",
    "3. **Predict New Points:**\n",
    "   - For a given input $x$, the prediction is:\n",
    "     $$\n",
    "     f(x) = \\sum \\alpha_i K(x_i, x) + b\n",
    "     $$\n",
    "     Where $\\alpha_i$ are the Lagrange multipliers and $K$ is the kernel function.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Summary**\n",
    "\n",
    "- SVR aims to predict values within an $\\epsilon$-margin of tolerance.\n",
    "- Support Vectors are the key data points that influence the regression function.\n",
    "- The optimization balances the trade-off between model complexity (flatness) and fitting the data.\n",
    "- Kernels allow SVR to handle non-linear data effectively.\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df8afd-30ce-4aea-9831-54c1e0333a8e",
   "metadata": {},
   "source": [
    "# Support Vector Regression (SVR) Formulas and Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae8370-9b30-4174-88b2-8d15db4a63dd",
   "metadata": {},
   "source": [
    "1. **SVR Objective**:\n",
    "   $$\\text{Minimize: } \\frac{1}{2} \\|w\\|^2$$\n",
    "   - **Subject to:**\n",
    "     $$\\begin{aligned}\n",
    "     &y_i - \\langle w, x_i \\rangle - b \\leq \\epsilon, \\\\\n",
    "     &\\langle w, x_i \\rangle + b - y_i \\leq \\epsilon.\n",
    "     \\end{aligned}$$\n",
    "   - **Explanation**:\n",
    "     - $w$: Weight vector (defines the direction of the hyperplane).\n",
    "     - $x_i$: Input features.\n",
    "     - $y_i$: Target value.\n",
    "     - $b$: Bias term.\n",
    "     - $\\epsilon$: Epsilon-tube margin (error tolerance).\n",
    "\n",
    "2. **Slack Variables** (for soft margins):\n",
    "   $$\\text{Minimize: } \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n (\\xi_i + \\xi_i^*)$$\n",
    "   - **Subject to:**\n",
    "     $$\\begin{aligned}\n",
    "     &y_i - \\langle w, x_i \\rangle - b \\leq \\epsilon + \\xi_i, \\\\\n",
    "     &\\langle w, x_i \\rangle + b - y_i \\leq \\epsilon + \\xi_i^*, \\\\\n",
    "     &\\xi_i, \\xi_i^* \\geq 0.\n",
    "     \\end{aligned}$$\n",
    "   - **Explanation**:\n",
    "     - $\\xi_i, \\xi_i^*$: Slack variables for points outside the $\\epsilon$ margin.\n",
    "     - $C$: Regularization parameter controlling the trade-off between minimizing $\\|w\\|^2$ and the margin violation.\n",
    "\n",
    "3. **Kernel Trick** (For non-linear SVR):\n",
    "   $$\\langle w, x_i \\rangle = \\sum_{j=1}^n \\alpha_j K(x_i, x_j)$$\n",
    "   - **Explanation**:\n",
    "     - $K(x_i, x_j)$: Kernel function (e.g., linear, polynomial, RBF) to compute similarity in high-dimensional space.\n",
    "     - $\\alpha_j$: Lagrange multipliers (learned during optimization).\n",
    "\n",
    "### SVR Workflow:\n",
    "1. Define the error tolerance $\\epsilon$.\n",
    "2. Choose a kernel function $K(x_i, x_j)$ (e.g., linear, polynomial, or RBF).\n",
    "3. Solve the optimization problem to find $w$, $b$, and slack variables $\\xi_i, \\xi_i^*$.\n",
    "4. Use the model to predict outputs:\n",
    "   $$\\hat{y} = \\langle w, x \\rangle + b$$\n",
    "   Or, with kernels:\n",
    "   $$\\hat{y} = \\sum_{j=1}^n \\alpha_j K(x, x_j) + b$$\n",
    "\n",
    "### Key Parameters:\n",
    "- **$\\epsilon$**: Defines the margin of tolerance where no penalty is given for errors.\n",
    "- **$C$**: Regularization parameter that balances margin size and error tolerance.\n",
    "- **Kernel**: Transforms data into higher-dimensional space for non-linear regression.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b104d3f4-8551-426c-8c32-5716b87724fd",
   "metadata": {},
   "source": [
    "# How Does SVR (Support Vector Regression) Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb2020b-306f-408a-80ce-6a5f6fc6db2d",
   "metadata": {},
   "source": [
    "Support Vector Regression (SVR) is a regression technique based on **Support Vector Machines (SVM)**. It aims to find a function that **fits the data while ignoring small errors** using a margin (ε-tube). Unlike other regression methods that minimize the error directly, SVR tries to keep most predictions **within a margin (ε)** while penalizing large deviations.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step-by-Step Working of SVR**\n",
    "1️⃣ **Map Input Data to a Higher-Dimensional Space**  \n",
    "   - If the data is **non-linear**, SVR **transforms it using kernel functions** to a higher-dimensional space where it is easier to fit a hyperplane.\n",
    "\n",
    "2️⃣ **Define an ε-Tube Around the Regression Line**  \n",
    "   - The goal is to find a function $f(x)$ that **stays within an error margin (ε)** around the actual values.\n",
    "   - Predictions inside this margin **are not penalized**.\n",
    "\n",
    "3️⃣ **Minimize Complexity While Allowing Some Outliers**  \n",
    "   - Unlike ordinary regression, **SVR does not minimize just the squared error**.\n",
    "   - Instead, it **minimizes a combination of model complexity and large deviations (slack variables ξ)**.\n",
    "\n",
    "4️⃣ **Find the Optimal Hyperplane Using Support Vectors**  \n",
    "   - SVR selects a subset of data points (support vectors) **that define the margin**.\n",
    "   - Other points inside the ε-tube are ignored.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Formulation of SVR**\n",
    "### **Objective Function** (Minimizing Complexity & Large Errors)\n",
    "\n",
    "$\n",
    "\\min_{w, b} \\frac{1}{2} ||w||^2 + C \\sum (\\xi_i + \\xi_i^*)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ ||w||^2 $ → Keeps the function simple (regularization).\n",
    "- $ C $ → Controls the trade-off between **simplicity** and **outliers**.\n",
    "- $ \\xi, \\xi^* $ → Slack variables for points outside the ε-margin.\n",
    "\n",
    "### **Constraints** (Ensuring Predictions Stay in the Margin)\n",
    "\n",
    "$\n",
    "y_i - (w \\cdot x_i + b) \\leq \\epsilon + \\xi_i\n",
    "$\n",
    "\n",
    "$\n",
    "(w \\cdot x_i + b) - y_i \\leq \\epsilon + \\xi_i^*\n",
    "$\n",
    "\n",
    "- If a point is within the ε-tube, $ \\xi = 0 $ (no penalty).\n",
    "- If a point **exceeds ε**, it incurs a penalty proportional to $ \\xi $.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Hyperparameters in SVR**\n",
    "| **Hyperparameter** | **Description** | **Effect** |\n",
    "|--------------------|----------------|------------|\n",
    "| `C` (Regularization) | Controls trade-off between **model complexity** and **margin violations** | Higher **C** → Less margin, more overfitting |\n",
    "| `epsilon` (ε-Tube) | Defines the margin where no penalty is applied | Higher **ε** → Fewer support vectors, simpler model |\n",
    "| `kernel` | Maps data into higher dimensions | `linear`, `poly`, `rbf`, `sigmoid` |\n",
    "| `degree` (for `poly` kernel) | Defines the degree of the polynomial kernel | Only used for `poly` kernel |\n",
    "| `gamma` (for `rbf` and `poly` kernels) | Controls how far influence of a single point extends | Higher **gamma** → More complex model |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d21579-f4c3-4652-ab30-a64c072d1513",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Implementation in Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e09e9c38-f5ee-49f8-8ef2-45568c063863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.54059123]\n",
      " [3.27487379]\n",
      " [4.7916493 ]\n",
      " [6.2083507 ]\n",
      " [7.72512621]\n",
      " [8.45940877]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([[1], [2], [3], [4], [5], [6]])\n",
    "y = np.array([2, 3, 5, 6, 8, 10])\n",
    "\n",
    "# Feature Scaling (important for SVR)\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Train SVR Model\n",
    "svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "svr.fit(X_scaled, y_scaled)\n",
    "\n",
    "# Predict\n",
    "y_pred = svr.predict(X_scaled)\n",
    "\n",
    "# Convert back to original scale\n",
    "y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "print(y_pred_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc71ca2f-34e8-4adc-ba06-9dd123eae569",
   "metadata": {},
   "source": [
    "\n",
    "## **When to Use SVR?**\n",
    "✅ **Small to Medium-Sized Datasets** – Works well when there are limited samples.  \n",
    "✅ **Data with Non-Linear Relationships** – Use kernels like `rbf` or `poly` for complex patterns.  \n",
    "✅ **When Robustness to Outliers is Needed** – The **ε-margin** ignores small errors.  \n",
    "\n",
    "---\n",
    "---\n",
    " 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7462cf-ce05-42d8-83a4-583c8d704414",
   "metadata": {},
   "source": [
    "# How Does SVC (Support Vector Classification) Work? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844107a6-16aa-4be7-8361-477c9f323b85",
   "metadata": {},
   "source": [
    "Support Vector Classification (**SVC**) is a classification algorithm based on **Support Vector Machines (SVM)**. It works by finding a **hyperplane** that best separates different classes in the dataset while maximizing the margin between them.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step-by-Step Working of SVC**\n",
    "### **1️⃣ Transform Data (if Needed)**\n",
    "- If the data is **linearly separable**, SVC works in its original space.\n",
    "- If the data is **non-linearly separable**, it uses **kernel tricks** to map data into a higher-dimensional space where a **linear separator can be found**.\n",
    "\n",
    "### **2️⃣ Find the Optimal Hyperplane**\n",
    "- SVC tries to find a **decision boundary (hyperplane)** that separates classes **with the maximum possible margin**.\n",
    "- The points closest to the hyperplane are called **support vectors**.\n",
    "\n",
    "### **3️⃣ Handle Non-Separable Cases Using Soft Margin**\n",
    "- If the data is not perfectly separable, SVC allows some misclassifications using a **soft margin** controlled by hyperparameter `C`.\n",
    "- `C` controls the trade-off between **maximizing the margin** and **minimizing classification errors**.\n",
    "\n",
    "### **4️⃣ Use Kernel Trick for Non-Linear Cases**\n",
    "- If a **linear boundary** cannot separate the data, **SVC applies kernel functions** to transform it into a higher-dimensional space.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Formulation of SVC**\n",
    "### **Objective Function (Maximizing the Margin)**\n",
    "\n",
    "$\n",
    "\\min_{w, b} \\frac{1}{2} ||w||^2 + C \\sum \\xi_i\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ ||w||^2 $ → Ensures a **maximum margin** between classes.\n",
    "- $ C $ → Regularization parameter (controls misclassification).\n",
    "- $ \\xi_i $ → Slack variables (allow misclassified points).\n",
    "\n",
    "### **Constraints**\n",
    "For correctly classified points:\n",
    "\n",
    "$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i\n",
    "$\n",
    "Where:\n",
    "- $ y_i $ is the actual class label (+1 or -1).\n",
    "- $ w $ is the weight vector.\n",
    "- $ x_i $ is the input feature.\n",
    "- $ b $ is the bias.\n",
    "- $ \\xi_i $ allows some points to be within the margin.\n",
    "\n",
    "---\n",
    "\n",
    "## **Hyperparameters in SVC**\n",
    "| **Hyperparameter** | **Description** | **Effect** |\n",
    "|--------------------|----------------|------------|\n",
    "| `C` (Regularization) | Controls trade-off between **margin size** and **misclassifications** | Higher **C** → Less margin, more overfitting |\n",
    "| `kernel` | Maps data into higher dimensions | `linear`, `poly`, `rbf`, `sigmoid` |\n",
    "| `gamma` (for `rbf`, `poly`, `sigmoid`) | Controls how much influence a single point has | Higher **gamma** → More complex model |\n",
    "| `degree` (for `poly` kernel) | Defines the polynomial degree for `poly` kernel | Used when `kernel='poly'` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c5441-7ecd-4506-b2a5-708730909023",
   "metadata": {},
   "source": [
    "## **Implementation in Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "571f2e22-3260-47f6-b6fe-5d2dcbc51652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate Sample Data\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, \n",
    "                           n_redundant=0, n_classes=2, random_state=42)\n",
    "# Split into Train & Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVC Model\n",
    "svc = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8157f0ee-3cad-4241-9239-8a5e3229f958",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## **When to Use SVC?**\n",
    "✅ **When the Data is Linearly Separable** → Use `kernel='linear'`.  \n",
    "✅ **When the Data is Non-Linear** → Use `kernel='rbf'` or `kernel='poly'`.  \n",
    "✅ **When You Have a Small to Medium Dataset** → Works well with smaller datasets.  \n",
    "✅ **When You Need a Robust Model with Margin-Based Classification** → Works better than logistic regression for some datasets.\n",
    "\n",
    "---\n",
    "---\n",
    "🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
