{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2d095f-0d1b-4a76-b4a9-158b7ad94772",
   "metadata": {},
   "source": [
    "**Bagging** and **Boosting** are two of the most popular ensemble learning techniques used to improve the performance of machine learning models. Both methods combine multiple \"weak learners\" (models that perform slightly better than random guessing) to create a \"strong learner\" (a highly accurate model). However, they differ significantly in how they build and combine these weak learners.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Bagging (Bootstrap Aggregating)**\n",
    "### **Definition:**\n",
    "Bagging is an ensemble method that trains multiple models (weak learners) independently on different subsets of the dataset and combines their predictions to reduce variance and improve accuracy.\n",
    "\n",
    "### **Key Concepts:**\n",
    "- **Bootstrap Sampling:**\n",
    "  - Random subsets of the original dataset are created with replacement (some samples may be repeated in a subset).\n",
    "  - Each subset is used to train a separate model.\n",
    "  \n",
    "- **Parallel Training:**\n",
    "  - Models are trained independently (in parallel) and do not depend on each other.\n",
    "\n",
    "- **Aggregation:**\n",
    "  - Predictions from all models are combined:\n",
    "    - For classification: Majority voting (e.g., class with the most votes is chosen).\n",
    "    - For regression: Averaging predictions from all models.\n",
    "\n",
    "### **Advantages of Bagging:**\n",
    "1. **Reduces Overfitting:**\n",
    "   - It reduces variance by averaging multiple predictions, making the model more robust to noise.\n",
    "2. **Handles Complex Models:**\n",
    "   - Bagging works well with high-variance models like decision trees.\n",
    "3. **Parallelizable:**\n",
    "   - Training can be done independently, making it computationally efficient on modern hardware.\n",
    "\n",
    "### **Disadvantages of Bagging:**\n",
    "1. **Does Not Reduce Bias:**\n",
    "   - If individual models are biased, bagging will not fix it. It primarily addresses variance.\n",
    "2. **Large Number of Models:**\n",
    "   - Bagging requires training multiple models, which can be computationally expensive.\n",
    "\n",
    "### **Example Algorithms Using Bagging:**\n",
    "- Random Forest: Combines decision trees using bagging and feature randomness.\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Bagging:**\n",
    "- When your base model overfits the data (high variance).\n",
    "- When you have enough computational resources to train multiple models in parallel.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Boosting**\n",
    "### **Definition:**\n",
    "Boosting is an ensemble method that builds models sequentially, where each new model focuses on correcting the errors made by the previous models. The goal is to reduce both bias and variance.\n",
    "\n",
    "### **Key Concepts:**\n",
    "- **Sequential Training:**\n",
    "  - Models are trained one after the other. Each model is built to improve on the mistakes of the previous one.\n",
    "\n",
    "- **Weighted Data:**\n",
    "  - After each iteration, more weight is assigned to the samples that were incorrectly predicted, so the next model focuses on those difficult examples.\n",
    "\n",
    "- **Weighted Aggregation:**\n",
    "  - The final prediction is made by combining all models' predictions, typically weighted by their accuracy.\n",
    "\n",
    "### **Advantages of Boosting:**\n",
    "1. **Reduces Bias and Variance:**\n",
    "   - Boosting addresses both underfitting (bias) and overfitting (variance).\n",
    "2. **Improves Weak Models:**\n",
    "   - Boosting converts weak learners into a strong learner.\n",
    "3. **Effective for Complex Datasets:**\n",
    "   - Boosting works well on datasets with non-linear decision boundaries.\n",
    "\n",
    "### **Disadvantages of Boosting:**\n",
    "1. **Sensitive to Noise:**\n",
    "   - Boosting can overfit if the data contains a lot of noise, as it tries to fit every data point, including outliers.\n",
    "2. **Sequential Nature:**\n",
    "   - Boosting models are trained sequentially, which makes the process slower compared to bagging.\n",
    "\n",
    "### **Example Algorithms Using Boosting:**\n",
    "- AdaBoost (Adaptive Boosting)\n",
    "- Gradient Boosting Machines (GBM)\n",
    "- XGBoost (Extreme Gradient Boosting)\n",
    "- LightGBM\n",
    "- CatBoost\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Boosting:**\n",
    "- When your base model underfits the data (high bias).\n",
    "- When you want a more accurate and generalized model, and computational efficiency is not a primary concern.\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison of Bagging and Boosting**\n",
    "\n",
    "| **Feature**                 | **Bagging**                                      | **Boosting**                                      |\n",
    "|-----------------------------|------------------------------------------------|------------------------------------------------|\n",
    "| **Goal**                    | Reduce variance (overfitting).                  | Reduce bias and variance (underfitting/overfitting). |\n",
    "| **Training**                | Models are trained independently (in parallel). | Models are trained sequentially.                |\n",
    "| **Error Handling**          | Handles variance by averaging results.          | Corrects errors made by previous models.        |\n",
    "| **Overfitting**             | Less prone to overfitting.                      | Can overfit if not carefully tuned.             |\n",
    "| **Data Sampling**           | Uses bootstrap samples (with replacement).      | Focuses on misclassified samples by reweighting. |\n",
    "| **Example Algorithms**      | Random Forest                                   | AdaBoost, Gradient Boosting, XGBoost            |\n",
    "| **Speed**                   | Faster due to parallelization.                  | Slower because of sequential nature.            |\n",
    "\n",
    "---\n",
    "\n",
    "## **Example Implementation in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d861ca3e-04ff-4feb-bedd-41991f4137ce",
   "metadata": {},
   "source": [
    "### **Bagging Example (Random Forest):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68f5d06a-a1b4-4d02-a803-bcf8ff5858ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest (Bagging example)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy on test set:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd1acb-938d-4d94-a1f5-669515285b20",
   "metadata": {},
   "source": [
    "### **Boosting Example (AdaBoost):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "619e3f22-4b3e-45c1-a283-4758ac56b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aromal/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an AdaBoost Classifier (Boosting example)\n",
    "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy on test set:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba6f48-9569-4e15-8e85-f0c8b2565b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
