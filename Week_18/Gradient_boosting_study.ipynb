{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75dea774-e37c-4b62-b4e7-6391a4be5110",
   "metadata": {},
   "source": [
    "# Gradient Boosting Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7ce83d-1c6a-436c-a398-7c8d3343dea9",
   "metadata": {},
   "source": [
    "Gradient Boosting is an ensemble learning method that builds a strong predictive model by combining multiple weak models (typically decision trees) in a sequential manner. It minimizes the loss function by learning from the errors of previous models.\n",
    "\n",
    "---\n",
    "## **Step 1: Initialize the Model**\n",
    "We start by initializing the model with a simple prediction, usually the **mean** (for regression) or **log-odds** (for classification).\n",
    "For regression:\n",
    "\n",
    "$F_0(x) = \\arg\\min_{\\gamma} \\sum_{i=1}^{n} L(y_i, \\gamma)$\n",
    "\n",
    "where:\n",
    "- $F_0(x)$ is the initial model (often a constant),\n",
    "- $L(y_i, \\gamma)$ is the loss function (e.g., Mean Squared Error),\n",
    "- $y_i$ are the actual values.\n",
    "---\n",
    "## **Step 2: Compute Residuals (Negative Gradients)**\n",
    "At each iteration $m$, compute the negative gradient (pseudo-residuals), which points in the direction of the steepest descent of the loss function.\n",
    "\n",
    "$r_{im} = - \\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x) = F_{m-1}(x)}$\n",
    "\n",
    "where:\n",
    "- $r_{im}$ are the residuals (negative gradients),\n",
    "- $F_{m-1}(x)$ is the prediction from the previous iteration.\n",
    "\n",
    "For Mean Squared Error (MSE):\n",
    "\n",
    "$r_{im} = y_i - F_{m-1}(x_i)$\n",
    "\n",
    "For Log Loss in classification:\n",
    "\n",
    "$r_{im} = y_i - p_{m-1}(x_i), \\quad p_{m-1}(x_i) = \\frac{1}{1 + e^{-F_{m-1}(x_i)}}$\n",
    "\n",
    "---\n",
    "## **Step 3: Fit a Weak Learner (Decision Tree)**\n",
    "Fit a weak learner (e.g., a small decision tree) to predict the residuals:\n",
    "\n",
    "$h_m(x) = \\arg\\min_{h} \\sum_{i=1}^{n} (r_{im} - h(x_i))^2$\n",
    "\n",
    "where:\n",
    "- $h_m(x)$ is the weak learner (decision tree),\n",
    "- It tries to approximate the residuals $r_{im}$.\n",
    "---\n",
    "## **Step 4: Compute Step Size (Shrinkage)**\n",
    "Find the best step size $\\gamma_m$ by minimizing the loss:\n",
    "\n",
    "$\\gamma_m = \\arg\\min_{\\gamma} \\sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))$\n",
    "\n",
    "This step ensures that we move in the optimal direction while updating our model.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5: Update the Model**\n",
    "Update the model by adding the new weak learner:\n",
    "\n",
    "$F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)$\n",
    "\n",
    "where:\n",
    "- $F_m(x)$ is the updated model,\n",
    "- $h_m(x)$ is the weak learner,\n",
    "- $\\gamma_m$ is the learning rate (shrinkage factor).\n",
    "---\n",
    "## **Step 6: Repeat for Multiple Iterations**\n",
    "Repeat **Steps 2 to 5** for $M$ iterations until convergence or a stopping criterion is met.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 7: Final Prediction**\n",
    "After $M$ iterations, the final model is:\n",
    "\n",
    "$F_M(x) = F_0(x) + \\sum_{m=1}^{M} \\gamma_m h_m(x)$\n",
    "\n",
    "---\n",
    "## **Advantages of Gradient Boosting**\n",
    "✅ Handles non-linear relationships well  \n",
    "✅ Works with both regression and classification  \n",
    "✅ Reduces bias and variance effectively  \n",
    "✅ Highly customizable with hyperparameters like learning rate, tree depth, and loss functions  \n",
    "\n",
    "---\n",
    "## **Disadvantages of Gradient Boosting**\n",
    "❌ Can be slow due to sequential learning  \n",
    "❌ Prone to overfitting if not regularized  \n",
    "❌ Requires careful tuning of hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
