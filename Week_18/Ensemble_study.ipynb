{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb03efa-c88e-43df-9d8e-ca204417b7ae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### What Are Ensemble Techniques?\n",
    "\n",
    "**Ensemble techniques** in machine learning are methods that combine the predictions of multiple models (referred to as \"weak learners\" or \"base models\") to produce a more accurate and robust predictive model. These techniques rely on the idea that combining multiple models can offset their individual weaknesses, leading to improved overall performance.\n",
    "\n",
    "The ensemble models can be:\n",
    "1. **Homogeneous**: Using the same type of base models (e.g., multiple decision trees in Random Forest).\n",
    "2. **Heterogeneous**: Using different types of base models (e.g., combining a decision tree, SVM, and neural network).\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Ensemble Techniques\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating)**:\n",
    "   - Each model is trained on a random subset of the data (with replacement), and their predictions are averaged (regression) or voted on (classification).\n",
    "   - Example: **Random Forest**.\n",
    "\n",
    "2. **Boosting**:\n",
    "   - Models are built sequentially, and each new model focuses on correcting the errors of the previous ones.\n",
    "   - Examples: **AdaBoost**, **Gradient Boosting**, **XGBoost**.\n",
    "\n",
    "3. **Stacking**:\n",
    "   - Combines predictions from multiple models using a \"meta-model\" that learns how to best combine their outputs.\n",
    "\n",
    "4. **Voting**:\n",
    "   - Combines predictions of models by majority voting (classification) or averaging (regression).\n",
    "\n",
    "5. **Blending**:\n",
    "   - Similar to stacking but uses a hold-out dataset for the meta-model instead of cross-validation.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Are Ensemble Techniques Used?\n",
    "\n",
    "1. **Increased Accuracy**:\n",
    "   - Ensemble models often outperform individual models because they reduce overfitting, variance, and bias.\n",
    "\n",
    "2. **Reduced Overfitting**:\n",
    "   - By combining multiple models, ensembles smooth out predictions and reduce the risk of overfitting on training data.\n",
    "\n",
    "3. **Improved Generalization**:\n",
    "   - Ensembles generalize better on unseen data, as they capture diverse patterns.\n",
    "\n",
    "4. **Robustness**:\n",
    "   - They are less sensitive to noise and errors in the dataset.\n",
    "\n",
    "5. **Handles Complexity**:\n",
    "   - Ensemble techniques are suitable for complex datasets where a single model struggles to perform well.\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Applications\n",
    "- Fraud detection\n",
    "- Medical diagnosis\n",
    "- Recommendation systems\n",
    "- Predictive modeling in finance\n",
    "- Sentiment analysis in NLP\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbf7377-fcb2-4303-a135-44583ce76ba3",
   "metadata": {},
   "source": [
    "# How Stacking Combines Multiple Models in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fdb9d3-8323-407c-abf8-b1b21603cf3b",
   "metadata": {},
   "source": [
    "**Stacking** (Stacked Generalization) is an **ensemble learning technique** that combines multiple models (**base learners**) to improve predictive performance. It works by training multiple models and then using a **meta-model** (blender) to combine their outputs.\n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ”¹ Step-by-Step Process of Stacking**\n",
    "\n",
    "### **Step 1: Train Multiple Base Models (Level-0 Models)**\n",
    "- Train different machine learning models (e.g., Decision Tree, SVM, Random Forest, etc.).\n",
    "- These models learn independently from the training data.\n",
    "- Each model makes predictions on the same dataset.\n",
    "\n",
    "### **Step 2: Generate Predictions from Base Models**\n",
    "- The predictions from all base models are collected.\n",
    "- These predictions become the new dataset for the **meta-model**.\n",
    "\n",
    "### **Step 3: Train a Meta-Model (Level-1 Model)**\n",
    "- A new model (**meta-learner**) is trained on the predictions of the base models.\n",
    "- The meta-model learns how to combine the base models' predictions effectively.\n",
    "- Common meta-models include **Linear Regression, Logistic Regression, or another powerful ML model**.\n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ”¹ Stacking vs. Other Ensemble Methods**\n",
    "| Ensemble Method  | How It Works | Example |\n",
    "|------------------|-------------|---------|\n",
    "| **Bagging**     | Train multiple models on **different random subsets** and average their outputs | Random Forest |\n",
    "| **Boosting**    | Train models sequentially, correcting the previous model's errors | Gradient Boosting, AdaBoost |\n",
    "| **Stacking**    | Train multiple models and combine their outputs using a meta-model | Any combination of models |\n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ”¹ Example: Stacking with Scikit-Learn**\n",
    "```python\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svm', SVC(probability=True, kernel='rbf', random_state=42))\n",
    "]\n",
    "\n",
    "# Define meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Create stacking classifier\n",
    "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "# Train model\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = stacking_clf.score(X_test, y_test)\n",
    "print(\"Stacking Model Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ”¹ When to Use Stacking?**\n",
    "âœ… When you have **multiple strong models** and want to combine their strengths.  \n",
    "âœ… When individual models have **diverse decision boundaries**.  \n",
    "âœ… When you have enough computational power, as stacking can be expensive.  \n",
    "\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
