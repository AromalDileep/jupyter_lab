{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7fe0848-865f-4fd4-8222-6456d2cf4780",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering vs K-Means: Advantages and Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1044b2d3-f375-4b7c-a19a-eacf830d86a7",
   "metadata": {},
   "source": [
    "Hierarchical Clustering and K-Means are two popular clustering techniques used in machine learning. Both have their strengths and weaknesses, and choosing between them depends on the dataset and the specific problem.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Hierarchical Clustering**\n",
    "Hierarchical clustering builds a tree-like structure (dendrogram) that represents nested clusters at different levels.\n",
    "\n",
    "### **Advantages**\n",
    "‚úÖ **No need to specify the number of clusters**  \n",
    "   - Unlike K-Means, hierarchical clustering does not require predefining the number of clusters.  \n",
    "‚úÖ **Produces a hierarchy of clusters**  \n",
    "   - Provides a dendrogram that helps in understanding the structure of the data.  \n",
    "‚úÖ **Works well with small datasets**  \n",
    "   - Can effectively cluster small datasets where distances between points matter.  \n",
    "‚úÖ **Useful for non-spherical clusters**  \n",
    "   - Can identify arbitrarily shaped clusters, unlike K-Means, which assumes spherical clusters.  \n",
    "\n",
    "### **Disadvantages**\n",
    "‚ùå **Computationally expensive**  \n",
    "   - Time complexity is **O(n¬≤) or O(n¬≥)**, making it slow for large datasets.  \n",
    "‚ùå **Sensitive to noise and outliers**  \n",
    "   - A few bad data points can significantly impact the hierarchy.  \n",
    "‚ùå **Hard to scale**  \n",
    "   - Not suitable for very large datasets due to high memory and computational requirements.  \n",
    "‚ùå **Merging/splitting decisions are final**  \n",
    "   - Once a merge or split is done, it cannot be undone, leading to potential errors.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. K-Means Clustering**\n",
    "K-Means partitions the dataset into **K** predefined clusters by minimizing the variance within clusters.\n",
    "\n",
    "### **Advantages**\n",
    "‚úÖ **Computationally efficient**  \n",
    "   - Runs in **O(n \\* k \\* d \\* i)**, where \\(n\\) is the number of points, \\(k\\) is the clusters, \\(d\\) is dimensions, and \\(i\\) is iterations. Much faster than hierarchical clustering.  \n",
    "‚úÖ **Scalable to large datasets**  \n",
    "   - Can handle large datasets efficiently.  \n",
    "‚úÖ **Works well when clusters are well-separated**  \n",
    "   - Performs well when clusters are compact and spherical.  \n",
    "‚úÖ **Flexible and easy to implement**  \n",
    "   - Works well in many real-world applications.  \n",
    "\n",
    "### **Disadvantages**\n",
    "‚ùå **Requires specifying \\( k \\) in advance**  \n",
    "   - The number of clusters must be predefined, which can be difficult if the structure is unknown.  \n",
    "‚ùå **Sensitive to initialization**  \n",
    "   - Poor initialization of cluster centroids can lead to suboptimal results.  \n",
    "‚ùå **Only works well for spherical clusters**  \n",
    "   - Assumes clusters are convex and isotropic, making it unsuitable for complex shapes.  \n",
    "‚ùå **Sensitive to outliers**  \n",
    "   - A few outliers can distort cluster centroids.  \n",
    "‚ùå **May converge to a local minimum**  \n",
    "   - Depending on initialization, K-Means can get stuck in a suboptimal clustering.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. When to Use Which?**\n",
    "| Feature | Hierarchical Clustering | K-Means |\n",
    "|---------|------------------------|---------|\n",
    "| **Dataset Size** | Small to medium (‚â§ 1000 points) | Large (Thousands to millions of points) |\n",
    "| **Computational Complexity** | High (O(n¬≤) or worse) | Low (O(n)) |\n",
    "| **Need for Predefined \\(k\\)** | No | Yes |\n",
    "| **Cluster Shape** | Works well for non-spherical clusters | Works best for spherical clusters |\n",
    "| **Scalability** | Not scalable for large data | Scales well |\n",
    "| **Handles Outliers** | Sensitive | Sensitive |\n",
    "| **Visualization** | Dendrogram helps visualize structure | Harder to visualize |\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "- **Use Hierarchical Clustering** when you have a small dataset and want to explore hierarchical relationships.  \n",
    "- **Use K-Means** when you have a large dataset and need a fast, scalable solution.  \n",
    "\n",
    "---\n",
    "---\n",
    "üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790fcbb5-373a-4632-85f5-8ddd7fbcf0d2",
   "metadata": {},
   "source": [
    "# DBSCAN vs K-Means: A Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbdad64-0e73-4333-8995-c37acdf6f7c7",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) and K-Means are both clustering algorithms but differ significantly in their approach and use cases.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Key Differences Between DBSCAN and K-Means**  \n",
    "\n",
    "| Feature        | DBSCAN (Density-Based Clustering) | K-Means (Centroid-Based Clustering) |\n",
    "|--------------|--------------------------------|--------------------------------|\n",
    "| **Cluster Shape** | Identifies arbitrarily shaped clusters | Works best for spherical clusters |\n",
    "| **Number of Clusters** | No need to specify in advance | Must specify $ k $ beforehand |\n",
    "| **Noise Handling** | Can identify noise and outliers | Does not handle noise well |\n",
    "| **Scalability** | Slower on large datasets (O(n log n) to O(n¬≤)) | Faster on large datasets (O(n)) |\n",
    "| **Works Well With** | Datasets with varying densities | Datasets with well-separated clusters |\n",
    "| **Sensitivity** | Sensitive to $ \\varepsilon $ (radius) and $ minPts $ (minimum points) | Sensitive to initialization and choice of $k$ |\n",
    "\n",
    "---\n",
    "\n",
    "## **2. How Each Algorithm Works**\n",
    "\n",
    "### **üìå DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**  \n",
    "DBSCAN groups points based on **density** rather than distance from centroids.\n",
    "\n",
    "### **Steps**  \n",
    "1. Select a random point and check how many neighbors are within a distance $ \\varepsilon $.  \n",
    "2. If the number of neighbors $ \\geq minPts $, a new cluster starts.  \n",
    "3. Expand the cluster by recursively adding density-reachable points.  \n",
    "4. If a point has fewer than $ minPts $ neighbors, it is labeled **noise**.  \n",
    "5. Repeat until all points are classified.  \n",
    "\n",
    "### **Advantages of DBSCAN**  \n",
    "‚úÖ **Does not require k as input**\n",
    "\n",
    "‚úÖ **Identifies outliers as noise**\n",
    "\n",
    "‚úÖ **Works with arbitrary cluster shapes**\n",
    "\n",
    "‚úÖ **Good at finding clusters of similar density**\n",
    "\n",
    "### **Disadvantages of DBSCAN**  \n",
    "\n",
    "‚ùå **Struggles with clusters of varying densities (when density varies significantly between different clusters)**\n",
    "\n",
    "‚ùå **Choosing $ \\varepsilon $ and $ minPts $ parameters can be difficult**\n",
    "\n",
    "‚ùå **Computationally expensive for high-dimensional data**\n",
    "\n",
    "---\n",
    "\n",
    "### **üìå K-Means (Centroid-Based Clustering)**  \n",
    "K-Means partitions data into $ k $ clusters by minimizing intra-cluster variance.\n",
    "\n",
    "### **Steps**  \n",
    "1. Choose $ k $ centroids randomly.  \n",
    "2. Assign each point to the nearest centroid.  \n",
    "3. Compute new centroids as the mean of assigned points.  \n",
    "4. Repeat until centroids no longer change.  \n",
    "\n",
    "### **Advantages of K-Means**  \n",
    "‚úÖ **Fast and scalable** for large datasets  \n",
    "‚úÖ **Easy to implement** and interpret  \n",
    "‚úÖ **Works well with convex, well-separated clusters**  \n",
    "\n",
    "### **Disadvantages of K-Means**  \n",
    "‚ùå **Must specify $ k $ beforehand**  \n",
    "‚ùå **Sensitive to initialization** (may converge to local minima)  \n",
    "‚ùå **Poor at handling outliers**  \n",
    "‚ùå **Fails for non-spherical clusters**  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. When to Use Which?**\n",
    "- **Use DBSCAN** when:\n",
    "  - You have clusters of irregular shape.\n",
    "  - You need to detect outliers.\n",
    "  - You don‚Äôt know the number of clusters in advance.\n",
    "  \n",
    "- **Use K-Means** when:\n",
    "  - You have large datasets with well-separated clusters.\n",
    "  - Your data follows a spherical distribution.\n",
    "  - You need a fast and scalable method.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "- **DBSCAN** is better for **density-based clustering**, irregular shapes, and outlier detection.  \n",
    "- **K-Means** is better for **well-separated, spherical clusters** and large-scale applications.\n",
    "\n",
    "---\n",
    "---\n",
    "üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db77f3-eea3-440c-9563-e9f85b71a45b",
   "metadata": {},
   "source": [
    "# Random Forest Classifier Hyperparameters  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415fe826-d984-413e-8171-c9e57a7298fe",
   "metadata": {},
   "source": [
    "The **Random Forest Classifier** is a powerful ensemble method used for classification tasks. You can adjust the following hyperparameters to optimize its performance:\n",
    "\n",
    "#### **1. n_estimators**  \n",
    "- **Definition**: Number of trees in the forest.  \n",
    "- **Effect**: More trees improve the performance but increase the computation time.  \n",
    "- **Typical Range**: 100 to 1000.\n",
    "\n",
    "#### **2. max_depth**  \n",
    "- **Definition**: Maximum depth of each tree.  \n",
    "- **Effect**: Controls overfitting. Deeper trees tend to overfit.  \n",
    "- **Typical Range**: None (default, trees are expanded until leaves are pure) or an integer (e.g., 5 to 20).\n",
    "\n",
    "#### **3. min_samples_split**  \n",
    "- **Definition**: Minimum number of samples required to split an internal node.  \n",
    "- **Effect**: Controls overfitting. Increasing it prevents the model from learning overly specific patterns.  \n",
    "- **Typical Range**: 2 to 10.\n",
    "\n",
    "#### **4. min_samples_leaf**  \n",
    "- **Definition**: Minimum number of samples required to be at a leaf node.  \n",
    "- **Effect**: Increasing it results in more generalization (reduces overfitting).  \n",
    "- **Typical Range**: 1 to 10.\n",
    "\n",
    "#### **5. max_features**  \n",
    "- **Definition**: The number of features to consider when looking for the best split.  \n",
    "- **Effect**: A lower value reduces overfitting but might underfit. A higher value could lead to overfitting.  \n",
    "- **Typical Range**: 'auto' (sqrt), 'log2', or integer (e.g., 5, 10).\n",
    "\n",
    "#### **6. bootstrap**  \n",
    "- **Definition**: Whether bootstrap samples are used when building trees.  \n",
    "- **Effect**: Setting it to `True` allows the model to sample with replacement, which generally improves performance.  \n",
    "- **Typical Values**: `True` or `False`.\n",
    "\n",
    "#### **7. criterion**  \n",
    "- **Definition**: The function to measure the quality of a split.  \n",
    "- **Effect**: 'gini' (Gini impurity) or 'entropy' (Information gain).  \n",
    "- **Typical Values**: `gini` or `entropy`.\n",
    "\n",
    "#### **8. oob_score**  \n",
    "- **Definition**: Whether to use out-of-bag samples to estimate the generalization accuracy.  \n",
    "- **Effect**: If `True`, it gives a more robust performance estimate.  \n",
    "- **Typical Values**: `True` or `False`.\n",
    "\n",
    "#### **9. n_jobs**  \n",
    "- **Definition**: The number of jobs to run in parallel for both `fit` and `predict`.  \n",
    "- **Effect**: Setting it to `-1` uses all available processors, speeding up the process.  \n",
    "- **Typical Values**: `-1` or integer.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d385af-9ca1-4b4f-a05d-53f91e2a0cec",
   "metadata": {},
   "source": [
    "# Random Forest Regressor Hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "716b9cf2-f7d5-4422-af54-ee71f00c167a",
   "metadata": {},
   "source": [
    "The **Random Forest Regressor** is used for regression tasks. The hyperparameters are similar to the classifier but are adapted for continuous value prediction.\n",
    "\n",
    "#### **1. n_estimators**  \n",
    "- **Definition**: Number of trees in the forest (similar to the classifier).  \n",
    "- **Typical Range**: 100 to 1000.\n",
    "\n",
    "#### **2. max_depth**  \n",
    "- **Definition**: Maximum depth of the tree (limits the number of splits).  \n",
    "- **Effect**: Prevents overfitting by limiting depth.  \n",
    "- **Typical Range**: None (default) or an integer (e.g., 5 to 20).\n",
    "\n",
    "#### **3. min_samples_split**  \n",
    "- **Definition**: Minimum number of samples required to split an internal node.  \n",
    "- **Effect**: Similar to classifier, it prevents overfitting when set higher.  \n",
    "- **Typical Range**: 2 to 10.\n",
    "\n",
    "#### **4. min_samples_leaf**  \n",
    "- **Definition**: Minimum number of samples required at a leaf node.  \n",
    "- **Effect**: Helps reduce overfitting.  \n",
    "- **Typical Range**: 1 to 10.\n",
    "\n",
    "#### **5. max_features**  \n",
    "- **Definition**: The number of features to consider when looking for the best split.  \n",
    "- **Effect**: Similar to classifier, it controls model complexity.  \n",
    "- **Typical Range**: 'auto', 'sqrt', 'log2', or integer values.\n",
    "\n",
    "#### **6. bootstrap**  \n",
    "- **Definition**: Whether bootstrap samples are used.  \n",
    "- **Effect**: `True` uses samples with replacement, generally improving performance.  \n",
    "- **Typical Values**: `True` or `False`.\n",
    "\n",
    "#### **7. criterion**  \n",
    "- **Definition**: The function to measure the quality of a split.  \n",
    "- **Effect**: 'mse' (Mean Squared Error) or 'mae' (Mean Absolute Error).  \n",
    "- **Typical Values**: `mse` or `mae`.\n",
    "\n",
    "#### **8. oob_score**  \n",
    "- **Definition**: Whether to use out-of-bag samples to estimate the generalization accuracy.  \n",
    "- **Effect**: It provides an unbiased evaluation.  \n",
    "- **Typical Values**: `True` or `False`.\n",
    "\n",
    "#### **9. n_jobs**  \n",
    "- **Definition**: The number of jobs to run in parallel for both `fit` and `predict`.  \n",
    "- **Effect**: Improves speed by utilizing multiple processors.  \n",
    "- **Typical Values**: `-1` (use all processors) or integer.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tuning Random Forest with GridSearchCV or RandomizedSearchCV**\n",
    "You can use `GridSearchCV` or `RandomizedSearchCV` for hyperparameter tuning to find the best combination of these hyperparameters.\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Example RandomForestClassifier hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "}\n",
    "\n",
    "# Apply GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "```\n",
    "\n",
    "### **Conclusion**\n",
    "- The **Random Forest Classifier** and **Random Forest Regressor** share similar hyperparameters, with a few differences in criteria (e.g., \"mse\" vs. \"gini\").\n",
    "- **Hyperparameter tuning** improves performance and helps find the optimal model.\n",
    "\n",
    "---\n",
    "---\n",
    "üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6816b8d6-01e9-4d68-9443-e17587358dbd",
   "metadata": {},
   "source": [
    "# Polynomial Regression Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343ed8a0-9ed6-4ef0-935f-010df7555060",
   "metadata": {},
   "source": [
    "Polynomial Regression is an extension of Linear Regression where we model the relationship between the independent variable(s) and the target using polynomial terms. It is implemented using **`PolynomialFeatures`** in `sklearn`.\n",
    "\n",
    "#### **üìå Key Hyperparameters for Polynomial Regression**  \n",
    "\n",
    "Since **Polynomial Regression** is just **Linear Regression with transformed features**, it does not have typical hyperparameters like trees or ensembles. However, the following parameters influence its performance:\n",
    "\n",
    "### **1. degree (Most Important Hyperparameter)**\n",
    "- **Definition**: The degree of the polynomial features.\n",
    "- **Effect**: Controls the complexity of the model.  \n",
    "  - **Low degree** ‚Üí Underfitting  \n",
    "  - **High degree** ‚Üí Overfitting  \n",
    "- **Typical Range**: 2 to 5 (higher values may cause overfitting).\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=3)  # Using a cubic polynomial\n",
    "X_poly = poly.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. include_bias**\n",
    "- **Definition**: Whether to include the bias term (intercept).\n",
    "- **Effect**: If `True`, includes an additional constant feature (1).  \n",
    "- **Typical Values**: `True` (default) or `False`.\n",
    "\n",
    "```python\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. interaction_only**\n",
    "- **Definition**: If `True`, only interaction terms are created (no squared or higher-power terms).\n",
    "- **Effect**: Reduces the complexity of the polynomial expansion.\n",
    "- **Typical Values**: `False` (default) or `True`.\n",
    "\n",
    "```python\n",
    "poly = PolynomialFeatures(degree=3, interaction_only=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Regularization Hyperparameters (if using Ridge or Lasso)**\n",
    "Since Polynomial Regression can easily **overfit**, we often use **Regularized Linear Regression (Ridge/Lasso/ElasticNet)**:\n",
    "\n",
    "#### **a) alpha (for Ridge & Lasso)**\n",
    "- **Definition**: Controls the regularization strength.\n",
    "- **Effect**:  \n",
    "  - **High `alpha`** ‚Üí More penalty (simpler model, avoids overfitting).  \n",
    "  - **Low `alpha`** ‚Üí Less penalty (fits data more closely).  \n",
    "- **Typical Range**: `0.001` to `10`.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_poly, y)\n",
    "```\n",
    "\n",
    "#### **b) l1_ratio (for ElasticNet)**\n",
    "- **Definition**: Controls the mix between Lasso (`L1`) and Ridge (`L2`) regularization.\n",
    "- **Effect**:  \n",
    "  - `0` ‚Üí Pure Ridge (`L2`).  \n",
    "  - `1` ‚Üí Pure Lasso (`L1`).  \n",
    "  - `0.5` ‚Üí A balance between both.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(X_poly, y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üí° Summary**\n",
    "| Hyperparameter        | Definition & Effect | Typical Range |\n",
    "|----------------------|-------------------|--------------|\n",
    "| **degree** (Main) | Polynomial degree | `2` to `5` |\n",
    "| **include_bias** | Adds a constant term | `True` or `False` |\n",
    "| **interaction_only** | Only creates interaction terms | `False` or `True` |\n",
    "| **alpha** (for Ridge/Lasso) | Regularization strength | `0.001` to `10` |\n",
    "| **l1_ratio** (for ElasticNet) | Mix of Ridge & Lasso | `0` to `1` |\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b6e267-371d-4467-80ad-3dcb7c8f9638",
   "metadata": {},
   "source": [
    "# Hyperparameters for SVR (Support Vector Regression) and SVC (Support Vector Classification)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120153e-1ee0-4a0d-87d3-91eb7f7dee18",
   "metadata": {},
   "source": [
    "Both **SVR (Support Vector Regression)** and **SVC (Support Vector Classification)** come from Support Vector Machines (SVMs) and share similar hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ Common Hyperparameters (for both SVC & SVR)**  \n",
    "\n",
    "### **1. `C` (Regularization Parameter)**\n",
    "- **Definition**: Controls the trade-off between achieving a low error and keeping the model simple.\n",
    "- **Effect**:\n",
    "  - **High `C`** ‚Üí More complex model, less margin, better fit to training data (risk of overfitting).\n",
    "  - **Low `C`** ‚Üí Simpler model, larger margin, allows misclassifications (risk of underfitting).\n",
    "- **Typical Range**: `0.001` to `1000` (default = `1`).\n",
    "\n",
    "```python\n",
    "svm = SVC(C=10)\n",
    "svr = SVR(C=1.0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. `kernel` (Choice of Kernel Function)**\n",
    "- **Definition**: Specifies the transformation of input data into a higher-dimensional space.\n",
    "- **Options**:\n",
    "  - **`linear`** ‚Üí Best for linearly separable data.\n",
    "  - **`poly`** ‚Üí Polynomial kernel (good for non-linear problems).\n",
    "  - **`rbf`** (default) ‚Üí Radial Basis Function (Gaussian) kernel, best for most cases.\n",
    "  - **`sigmoid`** ‚Üí Similar to neural networks.\n",
    "  \n",
    "```python\n",
    "svm = SVC(kernel=\"rbf\")  # Using Radial Basis Function (RBF) kernel\n",
    "svr = SVR(kernel=\"poly\", degree=3)  # Using Polynomial kernel with degree 3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. `gamma` (Only for Non-Linear Kernels)**\n",
    "- **Definition**: Controls the influence of a single training example.\n",
    "- **Effect**:\n",
    "  - **High `gamma`** ‚Üí Each point has high influence (model becomes more complex).\n",
    "  - **Low `gamma`** ‚Üí Each point has less influence (smoother decision boundary).\n",
    "- **Typical Range**: `scale`, `auto`, or manually set (e.g., `0.01` to `10`).\n",
    "\n",
    "```python\n",
    "svm = SVC(kernel=\"rbf\", gamma=0.1)\n",
    "svr = SVR(kernel=\"rbf\", gamma=\"scale\")  # \"scale\" is default\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. `degree` (Only for `poly` Kernel)**\n",
    "- **Definition**: Specifies the polynomial degree when using a polynomial kernel.\n",
    "- **Effect**:\n",
    "  - **Higher degree** ‚Üí More complex model (risk of overfitting).\n",
    "- **Typical Range**: `2` to `5`.\n",
    "\n",
    "```python\n",
    "svm = SVC(kernel=\"poly\", degree=3)\n",
    "svr = SVR(kernel=\"poly\", degree=2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Additional Hyperparameters (for SVC Only)**\n",
    "  \n",
    "### **5. `probability`**\n",
    "- **Definition**: Enables probability estimates.\n",
    "- **Effect**: If `True`, enables `predict_proba()` method.\n",
    "- **Default**: `False`.\n",
    "\n",
    "```python\n",
    "svm = SVC(probability=True)\n",
    "```\n",
    "\n",
    "### **6. `class_weight`**\n",
    "- **Definition**: Adjusts weights for imbalanced classes.\n",
    "- **Options**:\n",
    "  - `None` (default) ‚Üí All classes treated equally.\n",
    "  - `\"balanced\"` ‚Üí Adjusts weights based on class frequency.\n",
    "\n",
    "```python\n",
    "svm = SVC(class_weight=\"balanced\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ Additional Hyperparameters (for SVR Only)**\n",
    "\n",
    "### **7. `epsilon` (Epsilon-Tube in SVR)**\n",
    "- **Definition**: Specifies a margin of tolerance where predictions are considered correct.\n",
    "- **Effect**:\n",
    "  - **Small `epsilon`** ‚Üí More precise predictions but higher complexity.\n",
    "  - **Large `epsilon`** ‚Üí More tolerance for error, simpler model.\n",
    "- **Typical Range**: `0.001` to `1`.\n",
    "\n",
    "```python\n",
    "svr = SVR(epsilon=0.1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Summary Table**\n",
    "| Hyperparameter | Description | SVC | SVR | Typical Range |\n",
    "|---------------|-------------|-----|-----|---------------|\n",
    "| **C** | Regularization strength | ‚úÖ | ‚úÖ | `0.001 - 1000` |\n",
    "| **kernel** | Kernel function | ‚úÖ | ‚úÖ | `linear`, `rbf`, `poly`, `sigmoid` |\n",
    "| **gamma** | Influence of points (for `rbf`, `poly`) | ‚úÖ | ‚úÖ | `\"scale\"`, `0.01 - 10` |\n",
    "| **degree** | Polynomial degree (for `poly` kernel) | ‚úÖ | ‚úÖ | `2 - 5` |\n",
    "| **probability** | Enable probability estimates | ‚úÖ | ‚ùå | `True` or `False` |\n",
    "| **class_weight** | Handles imbalanced classes | ‚úÖ | ‚ùå | `\"balanced\"`, `None` |\n",
    "| **epsilon** | Tolerance margin for regression | ‚ùå | ‚úÖ | `0.001 - 1` |\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e987d21d-cae1-4ca4-a90d-48a305505491",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Regression Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea4403-fff3-4d87-9ee4-79607ae0ff2c",
   "metadata": {},
   "source": [
    "Regression models predict continuous values, so their evaluation focuses on measuring **how close predictions are to actual values**. Below are common **regression evaluation metrics**, their formulas, and when to use them.\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ Mean Absolute Error (MAE)**\n",
    "- **Definition**: Measures the average absolute difference between actual and predicted values.\n",
    "- **Formula**:  \n",
    "  $\n",
    "  MAE = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i |\n",
    "  $\n",
    "- **Pros**: Easy to interpret, gives equal weight to all errors.\n",
    "- **Cons**: Doesn't emphasize large errors.\n",
    "- **Best Use Case**: When you want a simple error measure that treats all errors equally.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"MAE:\", mae)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Mean Squared Error (MSE)**\n",
    "- **Definition**: Measures the average of squared differences between actual and predicted values.\n",
    "- **Formula**:  \n",
    "  $\n",
    "  MSE = \\frac{1}{n} \\sum_{i=1}^{n} ( y_i - \\hat{y}_i )^2\n",
    "  $\n",
    "- **Pros**: Penalizes larger errors more than smaller ones.\n",
    "- **Cons**: Not in the same unit as the target variable (because of squaring).\n",
    "- **Best Use Case**: When large errors should be penalized more.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(\"MSE:\", mse)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ Root Mean Squared Error (RMSE)**\n",
    "- **Definition**: Square root of MSE, gives error in the same unit as the target variable.\n",
    "- **Formula**:  \n",
    "  $\n",
    "  RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} ( y_i - \\hat{y}_i )^2}\n",
    "  $\n",
    "- **Pros**: Easier to interpret than MSE.\n",
    "- **Cons**: Still sensitive to large errors.\n",
    "- **Best Use Case**: When you need an interpretable error measure in the same unit as the target.\n",
    "\n",
    "```python\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "print(\"RMSE:\", rmse)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è‚É£ Mean Absolute Percentage Error (MAPE)**\n",
    "- **Definition**: Measures percentage error relative to the actual values.\n",
    "- **Formula**:  \n",
    "  $\n",
    "  MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100\n",
    "  $\n",
    "- **Pros**: Expresses error as a percentage, making it scale-independent.\n",
    "- **Cons**: Fails if `y_i = 0` and sensitive to small values.\n",
    "- **Best Use Case**: When the error should be represented as a percentage.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "print(\"MAPE:\", mape, \"%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5Ô∏è‚É£ R¬≤ Score (Coefficient of Determination)**\n",
    "- **Definition**: Measures how well the model explains the variance in the data.\n",
    "- **Formula**:  \n",
    "  $\n",
    "  R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
    "  $\n",
    "- **Pros**: Measures model fit; values closer to 1 indicate a better fit.\n",
    "- **Cons**: Can be misleading for non-linear relationships.\n",
    "- **Best Use Case**: When assessing the overall fit of the regression model.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(\"R¬≤ Score:\", r2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Summary of Regression Metrics**\n",
    "| Metric | Formula | Pros | Cons | Best Use Case |\n",
    "|--------|---------|------|------|--------------|\n",
    "| **MAE** | $ \\frac{1}{n} \\sum |y_i - \\hat{y}_i| $ | Easy to interpret | Doesn't emphasize large errors | When all errors are equally important |\n",
    "| **MSE** | $ \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2 $ | Penalizes large errors | Not in the same unit as target | When large errors should be penalized more |\n",
    "| **RMSE** | $ \\sqrt{MSE} $ | In the same unit as target | Sensitive to outliers | When you need an interpretable metric |\n",
    "| **MAPE** | $ \\frac{1}{n} \\sum \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100 $ | Expresses error as a percentage | Undefined for zero values | When the error should be in percentage |\n",
    "| **R¬≤ Score** | $ 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} $ | Measures goodness of fit | Can be misleading | When assessing overall model performance |\n",
    "\n",
    "---\n",
    "\n",
    "### **Which Metric to Use?**\n",
    "‚úÖ **If you want an absolute error measure** ‚Üí Use **MAE**  \n",
    "‚úÖ **If you want to penalize large errors more** ‚Üí Use **MSE** or **RMSE**  \n",
    "‚úÖ **If you need an interpretable error in the same unit** ‚Üí Use **RMSE**  \n",
    "‚úÖ **If you want a percentage error** ‚Üí Use **MAPE**  \n",
    "‚úÖ **If you want to measure overall model performance** ‚Üí Use **R¬≤ Score**  \n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5538e0-538d-4220-8da9-7c1b0514a9a1",
   "metadata": {},
   "source": [
    "# Why Do We Use Root Mean Squared Error (RMSE)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94f6850-1c3f-4e43-8da2-5dd15a4b11b2",
   "metadata": {},
   "source": [
    "**Root Mean Squared Error (RMSE)** is widely used in regression because it effectively measures the difference between predicted and actual values. Here‚Äôs why:\n",
    "\n",
    "---\n",
    "\n",
    "### **1Ô∏è‚É£ RMSE Penalizes Large Errors More**\n",
    "- RMSE squares the errors before averaging, giving **more weight to larger errors**.\n",
    "- This is useful when **large errors should be considered more significant** than small ones.\n",
    "\n",
    "üí° **Example:** If two models have similar mean errors, but one makes **larger occasional mistakes**, RMSE will highlight this difference.\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ RMSE Is in the Same Unit as the Target Variable**\n",
    "- Since RMSE is the **square root** of the Mean Squared Error (MSE), the final value is in **the same unit as the actual data**.\n",
    "- This makes RMSE **more interpretable** compared to MSE.\n",
    "\n",
    "üí° **Example:** If predicting house prices in **lakhs**, RMSE will also be in **lakhs**, making it easy to understand the model's average error.\n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ RMSE Works Well for Normally Distributed Errors**\n",
    "- Many real-world datasets have **errors that are normally distributed**.\n",
    "- RMSE **aligns well with such distributions**, making it a good metric for common regression problems.\n",
    "\n",
    "üí° **Example:** If prediction errors are randomly scattered around the true values, RMSE is a **reliable indicator** of model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **4Ô∏è‚É£ RMSE Is Differentiable for Optimization**\n",
    "- Since RMSE is based on squaring and summing differences, it is **smooth and differentiable**.\n",
    "- This makes it ideal for gradient-based optimization methods used in **machine learning models**.\n",
    "\n",
    "üí° **Example:** Gradient Descent can efficiently minimize RMSE, helping the model learn better.\n",
    "\n",
    "---\n",
    "\n",
    "### **5Ô∏è‚É£ When to Use RMSE?**\n",
    "‚úÖ **If you want a metric that strongly penalizes large errors**  \n",
    "‚úÖ **If you need results in the same unit as the target variable**  \n",
    "‚úÖ **If errors follow a normal distribution**  \n",
    "‚úÖ **If using a model that relies on gradient-based optimization**  \n",
    "\n",
    "---\n",
    "---\n",
    " üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866be545-7c98-4fe9-be4e-26bb6792a9c5",
   "metadata": {},
   "source": [
    "# GridSearchCV vs RandomizedSearchCV: Differences and When to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4358ac90-7b88-4dd2-bc98-5c3da2af8fe8",
   "metadata": {},
   "source": [
    "Both **GridSearchCV** and **RandomizedSearchCV** are **hyperparameter tuning techniques** in machine learning used to find the best combination of hyperparameters for a model.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 1. What is GridSearchCV?**  \n",
    "üìå **GridSearchCV** performs an **exhaustive search** over all possible hyperparameter combinations in a given range.\n",
    "\n",
    "### **How It Works:**\n",
    "1. You define a dictionary of hyperparameters and their possible values.\n",
    "2. GridSearchCV evaluates all possible combinations.\n",
    "3. It selects the best combination based on cross-validation performance.\n",
    "\n",
    "### **Example:**\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  \n",
    "    'max_depth': [None, 10, 20],  \n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "```\n",
    "\n",
    "### ‚úÖ **When to Use GridSearchCV?**\n",
    "- **If computational resources are sufficient** (since it tests all combinations).\n",
    "- **If the parameter space is small** (since exhaustive search is feasible).\n",
    "- **When accuracy is a priority over training time**.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ 2. What is RandomizedSearchCV?**  \n",
    "üìå **RandomizedSearchCV** randomly samples a subset of hyperparameter combinations instead of testing all possible values.\n",
    "\n",
    "### **How It Works:**\n",
    "1. You define a dictionary of hyperparameters and their value ranges.\n",
    "2. RandomizedSearchCV **randomly picks** a fixed number of combinations.\n",
    "3. It selects the best one based on cross-validation performance.\n",
    "\n",
    "### **Example:**\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 201, 50),  \n",
    "    'max_depth': [None, 10, 20],  \n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "random_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "```\n",
    "\n",
    "### ‚úÖ **When to Use RandomizedSearchCV?**\n",
    "- **If the hyperparameter space is large** (as it samples a subset instead of exhaustive search).\n",
    "- **If computational resources are limited** (as it speeds up the tuning process).\n",
    "- **If training time is a concern** (as fewer evaluations are performed).\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Key Differences Between GridSearchCV and RandomizedSearchCV**\n",
    "| Feature | GridSearchCV | RandomizedSearchCV |\n",
    "|---------|-------------|-------------------|\n",
    "| **Search Method** | Exhaustive (tests all combinations) | Random sampling of combinations |\n",
    "| **Computational Cost** | High (grows exponentially with more parameters) | Lower (controlled by `n_iter`) |\n",
    "| **Efficiency** | Best for small hyperparameter spaces | Best for large hyperparameter spaces |\n",
    "| **Exploration** | Explores entire space systematically | Explores randomly, may miss some values |\n",
    "| **Best Use Case** | When precision is more important than speed | When speed is more important than exhaustive search |\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Which One Should You Use?**\n",
    "‚úÖ Use **GridSearchCV** if:  \n",
    "- You have **a small set of hyperparameters**.  \n",
    "- You need **high accuracy and reliability**.  \n",
    "- You have **sufficient computing power**.  \n",
    "\n",
    "‚úÖ Use **RandomizedSearchCV** if:  \n",
    "- You have **a large range of hyperparameters**.  \n",
    "- You need **faster tuning**.  \n",
    "- You want to **quickly find a reasonably good model**.  \n",
    "\n",
    "---\n",
    "---\n",
    " üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d82761-f03c-465c-98be-bf426291464c",
   "metadata": {},
   "source": [
    "# When to Use Different Algorithms for Classification, Regression, and Clustering?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00917902-e3c2-4686-9d24-ed6fce7c302f",
   "metadata": {},
   "source": [
    "Different machine learning algorithms perform well under different conditions based on **dataset size, complexity, noise, interpretability, and computational efficiency**. Below is a **guideline for choosing algorithms** for **classification, regression, and clustering**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Classification Algorithms**\n",
    "| Algorithm | Best Use Case | Advantages | Limitations |\n",
    "|-----------|-------------|------------|-------------|\n",
    "| **Logistic Regression** | Binary classification, interpretability needed | Simple, interpretable, probability estimates | Assumes linear decision boundary |\n",
    "| **Decision Tree** | When rules/explainability are required | Easy to understand, handles non-linearity | Prone to overfitting |\n",
    "| **Random Forest** | High-dimensional data, non-linearity | Reduces overfitting, good accuracy | Computationally expensive |\n",
    "| **Support Vector Machine (SVM)** | Small to medium datasets, complex decision boundaries | Works well in high dimensions, good generalization | Computationally slow on large datasets |\n",
    "| **K-Nearest Neighbors (KNN)** | Non-parametric, small datasets | No training needed, simple | Slow for large datasets, sensitive to noise |\n",
    "| **Na√Øve Bayes** | Text classification, spam filtering | Fast, works well with small data | Assumes independence of features |\n",
    "| **Neural Networks (MLP, CNN, RNN)** | Deep learning tasks, large-scale data | Works well with complex relationships | Requires large datasets and tuning |\n",
    "| **XGBoost / Gradient Boosting** | Competitive performance, Kaggle competitions | High accuracy, handles missing values | Computationally expensive |\n",
    "\n",
    "üìå **Rule of Thumb**  \n",
    "‚úÖ **Small data (~few 100s samples)** ‚Üí **Logistic Regression, Na√Øve Bayes**  \n",
    "‚úÖ **Medium data (~few 1,000s samples)** ‚Üí **SVM, Decision Trees**  \n",
    "‚úÖ **Large data (~10,000+ samples)** ‚Üí **Random Forest, XGBoost, Neural Networks**  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Regression Algorithms**\n",
    "| Algorithm | Best Use Case | Advantages | Limitations |\n",
    "|-----------|-------------|------------|-------------|\n",
    "| **Linear Regression** | Simple relationships, interpretability | Easy to understand, fast | Assumes linearity |\n",
    "| **Ridge/Lasso Regression** | Avoiding overfitting, feature selection | Handles multicollinearity | Needs hyperparameter tuning |\n",
    "| **Polynomial Regression** | Non-linear relationships | Captures complex patterns | Overfits if the degree is too high |\n",
    "| **Decision Tree Regression** | Non-linear data, interpretability | Handles non-linearity | Prone to overfitting |\n",
    "| **Random Forest Regression** | High-dimensional data, non-linearity | Robust, reduces overfitting | Slower than Decision Tree |\n",
    "| **Support Vector Regression (SVR)** | Continuous target variables, complex relationships | Works well with small datasets | Computationally expensive |\n",
    "| **Gradient Boosting (XGBoost, LightGBM)** | High accuracy, structured data | Strong performance | Computationally expensive |\n",
    "| **Neural Networks (MLP, LSTM, etc.)** | Deep learning tasks, image/audio regression | Handles large datasets | Needs a lot of tuning and data |\n",
    "\n",
    "üìå **Rule of Thumb**  \n",
    "‚úÖ **Linear data** ‚Üí **Linear Regression**  \n",
    "‚úÖ **Small data with non-linearity** ‚Üí **Polynomial Regression, SVR**  \n",
    "‚úÖ **Large data with complex patterns** ‚Üí **Random Forest, XGBoost, Neural Networks**  \n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Clustering Algorithms**\n",
    "| Algorithm | Best Use Case | Advantages | Limitations |\n",
    "|-----------|-------------|------------|-------------|\n",
    "| **K-Means** | Large datasets, well-separated clusters | Fast, easy to implement | Sensitive to outliers, requires predefined K |\n",
    "| **Hierarchical Clustering** | Small datasets, hierarchical relationships | No need to predefine clusters | Computationally expensive |\n",
    "| **DBSCAN** | Density-based clustering, noise detection | Detects outliers, no need for K | Struggles with varying densities |\n",
    "| **Gaussian Mixture Model (GMM)** | Soft clustering, overlapping data | Probabilistic approach | Computationally expensive |\n",
    "| **Agglomerative Clustering** | Hierarchical relationships in data | No need to specify K | Doesn't scale well |\n",
    "| **Mean Shift** | Finding unknown cluster numbers | No need to specify clusters | Computationally heavy |\n",
    "\n",
    "üìå **Rule of Thumb**  \n",
    "‚úÖ **Large dataset (~10,000+ samples)** ‚Üí **K-Means**  \n",
    "‚úÖ **Small dataset (~few 1,000s samples)** ‚Üí **Hierarchical Clustering**  \n",
    "‚úÖ **Clusters with varying densities & outliers** ‚Üí **DBSCAN**  \n",
    "‚úÖ **Soft clustering / Probabilistic clustering** ‚Üí **GMM**  \n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Summary Table**\n",
    "| Task | Small Data | Medium Data | Large Data |\n",
    "|------|-----------|------------|------------|\n",
    "| **Classification** | Logistic Regression, Na√Øve Bayes | SVM, Decision Trees | Random Forest, XGBoost, Neural Networks |\n",
    "| **Regression** | Linear Regression, Polynomial Regression | SVR, Decision Trees | Random Forest, XGBoost, Neural Networks |\n",
    "| **Clustering** | Hierarchical Clustering, DBSCAN | K-Means, GMM | K-Means, DBSCAN |\n",
    "\n",
    "---\n",
    "---\n",
    " üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38fb35-7ec6-4997-b355-27946ba5ddef",
   "metadata": {},
   "source": [
    "# Na√Øve Bayes Algorithm ‚Äì Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e3462-3fc4-44bd-971f-cb45a53e72ef",
   "metadata": {},
   "source": [
    "Na√Øve Bayes is a **probabilistic classification algorithm** based on **Bayes' Theorem**. It is widely used in text classification (spam detection, sentiment analysis) and other applications requiring fast and scalable classification.\n",
    "\n",
    "---\n",
    "\n",
    "## **1Ô∏è‚É£ How Na√Øve Bayes Works**\n",
    "Na√Øve Bayes assumes that **features are independent given the class label** (hence the \"na√Øve\" assumption). This means it calculates the probability of a class given the input features using **Bayes' Theorem**:\n",
    "\n",
    "### **üìå Bayes' Theorem:**\n",
    "\n",
    "$\n",
    "P(C | X) = \\frac{P(X | C) \\cdot P(C)}{P(X)}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ P(C | X) $ = Probability of class **C** given the features **X** (**posterior probability**)\n",
    "- $ P(X | C) $ = Probability of features **X** given class **C** (**likelihood**)\n",
    "- $ P(C) $ = Probability of class **C** (**prior probability**)\n",
    "- $ P(X) $ = Probability of features **X** (**evidence**) (constant for all classes)\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ Steps of Na√Øve Bayes**\n",
    "1. **Calculate Prior Probabilities**:  \n",
    "   - Compute $ P(C) $, the proportion of each class in the dataset.\n",
    "\n",
    "2. **Calculate Likelihood for Each Feature**:  \n",
    "   - Compute $ P(X_i | C) $, the probability of each feature given a class.\n",
    "\n",
    "3. **Apply Bayes' Theorem**:  \n",
    "   - Compute $ P(C | X) $ for all classes and choose the class with the highest probability.\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ Types of Na√Øve Bayes**\n",
    "| Type | Use Case | Probability Distribution Assumption |\n",
    "|------|---------|--------------------------------|\n",
    "| **Gaussian Na√Øve Bayes** | Continuous data (e.g., Iris dataset) | Features follow a **Gaussian (Normal) distribution** |\n",
    "| **Multinomial Na√Øve Bayes** | Text classification (e.g., spam detection) | Features represent **word counts or term frequencies** |\n",
    "| **Bernoulli Na√Øve Bayes** | Binary feature data (e.g., sentiment analysis) | Features are **binary (0 or 1)** |\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è‚É£ Example: Na√Øve Bayes for Spam Detection**\n",
    "**Goal:** Classify an email as \"Spam\" or \"Not Spam\" based on words.  \n",
    "\n",
    "| Email | Word: \"offer\" | Word: \"win\" | Word: \"buy\" | Spam (1) or Not (0) |\n",
    "|-------|-------------|------------|------------|----------------------|\n",
    "| A     | 1           | 1          | 0          | Spam (1) |\n",
    "| B     | 0           | 1          | 1          | Spam (1) |\n",
    "| C     | 1           | 0          | 0          | Not Spam (0) |\n",
    "\n",
    "üìå **Steps:**\n",
    "1. Compute **Prior Probabilities**:\n",
    "   - $ P(\\text{Spam}) = 2/3 $, $ P(\\text{Not Spam}) = 1/3 $\n",
    "  \n",
    "2. Compute **Likelihoods** (probability of words in spam vs. not spam emails).\n",
    "\n",
    "3. Given a new email **\"offer win\"**, apply **Bayes' Theorem** to determine if it's **Spam or Not Spam**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5Ô∏è‚É£ Implementation in Python**\n",
    "```python\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample emails\n",
    "emails = [\"Buy now and win\", \"Limited time offer\", \"Meeting tomorrow\"]\n",
    "labels = [1, 1, 0]  # 1 = Spam, 0 = Not Spam\n",
    "\n",
    "# Convert text to numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(emails)\n",
    "\n",
    "# Train Na√Øve Bayes model\n",
    "model = MultinomialNB()\n",
    "model.fit(X, labels)\n",
    "\n",
    "# Predict on new email\n",
    "new_email = [\"Win a free offer now\"]\n",
    "new_X = vectorizer.transform(new_email)\n",
    "prediction = model.predict(new_X)\n",
    "print(\"Spam\" if prediction[0] == 1 else \"Not Spam\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6Ô∏è‚É£ Advantages & Disadvantages**\n",
    "### ‚úÖ **Advantages**\n",
    "‚úî **Fast and scalable**  \n",
    "‚úî **Works well with text data**  \n",
    "‚úî **Handles noisy data well**  \n",
    "‚úî **Requires small training data**  \n",
    "\n",
    "### ‚ùå **Disadvantages**\n",
    "‚úñ **Strong independence assumption** (features are not always independent)  \n",
    "‚úñ **Not suitable for correlated features**  \n",
    "‚úñ **Poor for datasets with complex relationships**  \n",
    "\n",
    "---\n",
    "\n",
    "## **7Ô∏è‚É£ When to Use Na√Øve Bayes?**\n",
    "‚úÖ **Text classification** (spam detection, sentiment analysis, document classification)  \n",
    "‚úÖ **Medical diagnosis** (predicting disease from symptoms)  \n",
    "‚úÖ **Real-time applications** (fraud detection, recommendation systems)  \n",
    "\n",
    "---\n",
    "---\n",
    " üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a49b2-1010-48f2-b62a-0079ae5a3621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
