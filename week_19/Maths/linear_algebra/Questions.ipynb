{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c88b2c-de79-4c54-b528-9d5856a2f70a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Basic Questions**  \n",
    "\n",
    "#### **1. What is a scalar, vector, matrix, and tensor?**  \n",
    "✅ **Answer:**  \n",
    "- **Scalar**: A single number (e.g., 5, -3.2).  \n",
    "- **Vector**: A 1D array of numbers (e.g., $\\mathbf{v} = [2, 5, -1]$).  \n",
    "- **Matrix**: A 2D array of numbers (e.g.,  \n",
    "  $\n",
    "  A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n",
    "  $\n",
    "- **Tensor**: A multi-dimensional array (e.g., a 3D array used in deep learning).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **2. What is the rank of a matrix?**  \n",
    "✅ **Answer:**  \n",
    "The **rank** of a matrix is the number of **linearly independent** rows or columns in the matrix. It determines the dimension of the vector space spanned by the matrix.  \n",
    "\n",
    "For example,  \n",
    "$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\end{bmatrix}\n",
    "$  \n",
    "The second column is just **2 times** the first column, so there is only **one independent column**, meaning **rank(A) = 1**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3. What is the determinant of a matrix, and what does it tell us?**  \n",
    "✅ **Answer:**  \n",
    "The **determinant** of a square matrix $A$ is a scalar value that represents whether the matrix is **invertible** or **singular**.  \n",
    "\n",
    "For a $2 \\times 2$ matrix:  \n",
    "$\n",
    "\\text{det}(A) = \\begin{vmatrix} a & b \\\\ c & d \\end{vmatrix} = ad - bc\n",
    "$  \n",
    "If **det(A) = 0**, the matrix is **singular (not invertible)**.  \n",
    "\n",
    "Example:  \n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad \\text{det}(A) = (1)(4) - (2)(3) = 4 - 6 = -2\n",
    "$  \n",
    "\n",
    "Since **det(A) ≠ 0**, the matrix is **invertible**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Intermediate Questions**  \n",
    "\n",
    "#### **4. What is an identity matrix, and why is it important?**  \n",
    "✅ **Answer:**  \n",
    "The **identity matrix** $ I_n $ is a **square matrix** with **1s on the diagonal** and **0s everywhere else**.  \n",
    "\n",
    "Example (for $3 \\times 3$ matrix):  \n",
    "$\n",
    "I_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n",
    "$  \n",
    "It acts as the **multiplicative identity** in matrix multiplication:  \n",
    "\n",
    "$\n",
    "A \\times I = I \\times A = A\n",
    "$  \n",
    "\n",
    "It is important because it plays the same role as **1 in scalar multiplication**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. What is matrix inversion, and when does a matrix have an inverse?**  \n",
    "✅ **Answer:**  \n",
    "The **inverse** of a square matrix $A$ is another matrix $A^{-1}$ such that:  \n",
    "\n",
    "$\n",
    "A \\times A^{-1} = I\n",
    "$  \n",
    "\n",
    "A matrix **has an inverse** if and only if its **determinant is non-zero** (**det(A) ≠ 0**).  \n",
    "\n",
    "For a **$2 \\times 2$ matrix**, the inverse is:  \n",
    "\n",
    "$\n",
    "A^{-1} = \\frac{1}{\\text{det}(A)} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n",
    "$  \n",
    "\n",
    "Example:  \n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad \\text{det}(A) = -2\n",
    "$  \n",
    "\n",
    "$\n",
    "A^{-1} = \\frac{1}{-2} \\begin{bmatrix} 4 & -2 \\\\ -3 & 1 \\end{bmatrix} = \\begin{bmatrix} -2 & 1 \\\\ 1.5 & -0.5 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. What is eigenvalue decomposition?**  \n",
    "✅ **Answer:**  \n",
    "Eigenvalue decomposition is the process of breaking down a matrix into **eigenvalues** and **eigenvectors**:  \n",
    "\n",
    "$\n",
    "A \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$  \n",
    "\n",
    "where:  \n",
    "\n",
    "- $ A $ is a square matrix.  \n",
    "- $ \\lambda $ is an **eigenvalue**.  \n",
    "- $ \\mathbf{v} $ is an **eigenvector**.  \n",
    "\n",
    "Example:  \n",
    "For  \n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix} 4 & -2 \\\\ 1 & 1 \\end{bmatrix}\n",
    "$  \n",
    "\n",
    "The eigenvalues are **5 and 0**, and the corresponding eigenvectors are found by solving **$ (A - \\lambda I) v = 0 $**.\n",
    "\n",
    "Eigenvalue decomposition is widely used in **PCA (Principal Component Analysis)** for dimensionality reduction.\n",
    "\n",
    "---\n",
    "\n",
    "Eigenvalues and eigenvectors are fundamental concepts in linear algebra.\n",
    "\n",
    "An eigenvector of a square matrix A is a non-zero vector v that, when multiplied by A, results in a scaled version of itself. The scaling factor is called the eigenvalue.\n",
    "\n",
    "Eigenvector is  a non zero vector that change only in scale (not direction) when a linear transfomation is applied to it\n",
    "\n",
    "An eigenvalue is indeed a scalar, but it indicates how much the eigenvector  is stretched or compressed during a transformation.\n",
    "\n",
    "When a matrix A transforms an eigenvector v, the result points in the same direction as the original vector, just scaled by the eigenvalue λ:\n",
    "\n",
    "A·v = λv\n",
    "\n",
    "- If λ = 2, the eigenvector is stretched to twice its original length\n",
    "- If λ = 0.5, the eigenvector is compressed to half its original length\n",
    "- If λ = -1, the eigenvector is reversed and maintains its length\n",
    "- If λ = 1, the eigenvector remains unchanged\n",
    "\n",
    "So the eigenvalue tells you the scaling factor applied to its corresponding eigenvector when the transformation occurs.\n",
    "\n",
    "Mathematically:\n",
    "A·v = λv\n",
    "\n",
    "Where:\n",
    "- A is a square matrix\n",
    "- v is the eigenvector\n",
    "- λ (lambda) is the eigenvalue\n",
    "\n",
    "In simpler terms, when you apply the transformation represented by matrix A to its eigenvector, the vector doesn't change direction - it only stretches or shrinks by the eigenvalue amount.\n",
    "\n",
    "Key properties:\n",
    "- For an n×n matrix, there are at most n eigenvalues\n",
    "- Eigenvectors corresponding to different eigenvalues are linearly independent\n",
    "- The determinant of a matrix equals the product of its eigenvalues\n",
    "- The trace (sum of diagonal elements) equals the sum of eigenvalues\n",
    "\n",
    "Practical applications include:\n",
    "- Principal Component Analysis in data science\n",
    "- Solving differential equations\n",
    "- Quantum mechanics (where eigenvalues represent observable quantities)\n",
    "- Stability analysis in dynamic systems\n",
    "- Google's PageRank algorithm\n",
    "\n",
    "To find eigenvalues, you solve the characteristic equation: det(A - λI) = 0\n",
    "Then, for each eigenvalue, you solve (A - λI)v = 0 to find the corresponding eigenvectors.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Questions**  \n",
    "\n",
    "#### **7. What is the Moore-Penrose pseudoinverse, and why is it useful?**  \n",
    "✅ **Answer:**  \n",
    "The **Moore-Penrose pseudoinverse** ($A^+$) is a **generalized inverse** used for **non-square** or **singular matrices** where normal inversion does not work. It is useful in **least squares regression** when solving  \n",
    "\n",
    "$\n",
    "Ax = b\n",
    "$  \n",
    "\n",
    "If $ A $ is **not invertible**, we use the pseudoinverse:  \n",
    "\n",
    "$\n",
    "x = A^+ b\n",
    "$  \n",
    "\n",
    "It is computed as:  \n",
    "\n",
    "$\n",
    "A^+ = (A^T A)^{-1} A^T\n",
    "$  \n",
    "\n",
    "This is used in **linear regression** to compute optimal weights.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. What is Singular Value Decomposition (SVD) and how is it used?**  \n",
    "✅ **Answer:**  \n",
    "**Singular Value Decomposition (SVD)** decomposes a matrix into three matrices:  \n",
    "\n",
    "$\n",
    "A = U \\Sigma V^T\n",
    "$  \n",
    "\n",
    "where:  \n",
    "\n",
    "- $ U $ and $ V^T $ are **orthogonal matrices**.  \n",
    "- $ \\Sigma $ is a **diagonal matrix** of **singular values**.  \n",
    "\n",
    "Uses:  \n",
    "- **Dimensionality reduction** (PCA).  \n",
    "- **Feature extraction** in NLP (Latent Semantic Analysis).  \n",
    "- **Solving linear systems** and **matrix compression**.\n",
    "\n",
    "---\n",
    "\n",
    "Singular Value Decomposition (SVD) is a way to break down any matrix into three simpler components. Think of it as factoring a number, but for matrices.\n",
    "\n",
    "For a matrix A, SVD gives you: A = USVᵀ where:\n",
    "\n",
    "- U is a matrix containing \"output directions\" (left singular vectors)\n",
    "- S is a diagonal matrix with \"stretching factors\" (singular values)\n",
    "- Vᵀ is a matrix containing \"input directions\" (right singular vectors, transposed)\n",
    "\n",
    "In simple terms, SVD reveals:\n",
    "1. Which directions in your input space are most important (V)\n",
    "2. How much stretching happens in each direction (S)\n",
    "3. Where those stretched directions end up in the output space (U)\n",
    "\n",
    "A real-world analogy: Imagine you're transforming a sphere of clay into an ellipsoid. SVD tells you:\n",
    "- Which directions to push the clay (V)\n",
    "- How hard to push in each direction (S)\n",
    "- Where those pushes send the clay (U)\n",
    "\n",
    "SVD is powerful because it works for ANY matrix (not just square ones) and reveals the most important components of your transformation. This makes it useful for data compression, noise reduction, recommender systems, and image processing.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9. What is the Frobenius norm of a matrix, and how is it computed?**  \n",
    "✅ **Answer:**  \n",
    "The **Frobenius norm** of a matrix $ A $ is a measure of its **magnitude**, computed as:  \n",
    "\n",
    "$\n",
    "\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}\n",
    "$  \n",
    "\n",
    "It is used for evaluating the **size of a matrix** in machine learning models.\n",
    "\n",
    "Example:  \n",
    "For  \n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\|A\\|_F = \\sqrt{1^2 + 2^2 + 3^2 + 4^2} = \\sqrt{30}\n",
    "$  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ebb850-31d0-4170-b028-f680107d093c",
   "metadata": {},
   "source": [
    "# Additional Questions\n",
    "---\n",
    "\n",
    "### **1. What is a transpose of a matrix?**\n",
    "✅ **Answer:**  \n",
    "The **transpose** of a matrix $ A $, denoted as $ A^T $, is obtained by **swapping rows and columns**.\n",
    "\n",
    "Example:  \n",
    "$\n",
    "A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\n",
    "$\n",
    "The **transpose** $ A^T $ is:\n",
    "$\n",
    "A^T = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}\n",
    "$\n",
    "📌 **Why is it useful?**  \n",
    "- In machine learning, **feature vectors** are often represented as **column vectors**, and transposing helps in matrix operations like dot products.  \n",
    "- Used in computing **covariance matrices** in statistics.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. What is a diagonal matrix?**\n",
    "✅ **Answer:**  \n",
    "A **diagonal matrix** is a square matrix where all the **non-diagonal elements** are **zero**.\n",
    "\n",
    "Example:\n",
    "$\n",
    "D = \\begin{bmatrix} 5 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}\n",
    "$\n",
    "📌 **Why is it useful?**  \n",
    "- **Fast computation**: Matrix operations (like multiplication, inversion) are much faster for diagonal matrices.  \n",
    "- **Eigenvalues** of a diagonal matrix are just the diagonal elements.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. What is the dot product of two vectors?**\n",
    "✅ **Answer:**  \n",
    "The **dot product** (also called the **inner product**) of two vectors **measures their similarity**.\n",
    "\n",
    "For two vectors $ \\mathbf{a} $ and $ \\mathbf{b} $:\n",
    "$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = a_1b_1 + a_2b_2 + \\dots + a_n b_n\n",
    "$\n",
    "Example:\n",
    "$\n",
    "\\mathbf{a} = [1, 2, 3], \\quad \\mathbf{b} = [4, 5, 6]\n",
    "$\n",
    "$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = (1 \\times 4) + (2 \\times 5) + (3 \\times 6) = 4 + 10 + 18 = 32\n",
    "$\n",
    "\n",
    "📌 **Why is it useful?**  \n",
    "- In **ML and deep learning**, dot products are used in **neural networks** and **cosine similarity** (for NLP tasks).  \n",
    "- **Projection of vectors** and angle measurement.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. What is matrix multiplication, and when is it possible?**\n",
    "✅ **Answer:**  \n",
    "**Matrix multiplication** is **only possible** if the **number of columns of the first matrix** matches the **number of rows of the second matrix**.\n",
    "\n",
    "If $ A $ is $ m \\times n $ and $ B $ is $ n \\times p $, then the result $ C = A \\times B $ will be an **$ m \\times p $ matrix**.\n",
    "\n",
    "Example:  \n",
    "$\n",
    "A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n",
    "$\n",
    "$\n",
    "C = A \\times B = \\begin{bmatrix} (1 \\times 5 + 2 \\times 7) & (1 \\times 6 + 2 \\times 8) \\\\ (3 \\times 5 + 4 \\times 7) & (3 \\times 6 + 4 \\times 8) \\end{bmatrix}\n",
    "$\n",
    "$\n",
    "C = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "📌 **Why is it useful?**  \n",
    "- Used in **deep learning** for **forward propagation** in neural networks.  \n",
    "- Essential in **linear transformations** in ML.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. What is the difference between row vector and column vector?**\n",
    "✅ **Answer:**  \n",
    "- A **row vector** is a **1 × n** matrix. Example:  \n",
    "  $\n",
    "  \\mathbf{v} = \\begin{bmatrix} 2 & 3 & 5 \\end{bmatrix}\n",
    "  $\n",
    "- A **column vector** is an **n × 1** matrix. Example:  \n",
    "  $\n",
    "  \\mathbf{v} = \\begin{bmatrix} 2 \\\\ 3 \\\\ 5 \\end{bmatrix}\n",
    "  $\n",
    "\n",
    "📌 **Why is it useful?**  \n",
    "- In ML, **feature vectors** are usually **column vectors**.  \n",
    "- Transposing row vectors helps in **dot products** and **matrix multiplication**.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. What is a symmetric matrix?**\n",
    "✅ **Answer:**  \n",
    "A **symmetric matrix** is a square matrix where $ A^T = A $, meaning the **diagonal remains the same**, and values are **mirrored** across the diagonal.\n",
    "\n",
    "Example:\n",
    "$\n",
    "A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 5 \\\\ 3 & 5 & 6 \\end{bmatrix}\n",
    "$\n",
    "Here, $ A_{ij} = A_{ji} $ (e.g., $ A_{12} = A_{21} $).\n",
    "\n",
    "📌 **Why is it useful?**  \n",
    "- Used in **covariance matrices** in statistics and ML.  \n",
    "- Helps in **eigenvalue decomposition**.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. What is an orthogonal matrix?**\n",
    "✅ **Answer:**  \n",
    "A **matrix is orthogonal** if its **transpose equals its inverse**:  \n",
    "$\n",
    "A^T A = I\n",
    "$\n",
    "Example:\n",
    "$\n",
    "A = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix}\n",
    "$\n",
    "📌 **Why is it useful?**  \n",
    "- Used in **dimensionality reduction** (PCA).  \n",
    "- Efficient in numerical computations.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. What is the trace of a matrix?**\n",
    "✅ **Answer:**  \n",
    "The **trace** of a square matrix $ A $ is the **sum of its diagonal elements**.\n",
    "\n",
    "Example:\n",
    "$\n",
    "A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 5 \\end{bmatrix}\n",
    "$\n",
    "$\n",
    "\\text{trace}(A) = 2 + 5 = 7\n",
    "$\n",
    "\n",
    "📌 **Why is it useful?**  \n",
    "- Used in **linear regression** and **covariance matrices**.  \n",
    "- Helps in **eigenvalue computations**.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. What is a unit vector?**\n",
    "✅ **Answer:**  \n",
    "A **unit vector** is a vector with a **magnitude (length) of 1**.\n",
    "\n",
    "If $ \\mathbf{v} = [x_1, x_2, ..., x_n] $, its unit vector is:\n",
    "$\n",
    "\\mathbf{u} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\n",
    "$\n",
    "where $ \\|\\mathbf{v}\\| $ is the **Euclidean norm**:\n",
    "$\n",
    "\\|\\mathbf{v}\\| = \\sqrt{x_1^2 + x_2^2 + ... + x_n^2}\n",
    "$\n",
    "\n",
    "Example:  \n",
    "For $ \\mathbf{v} = [3, 4] $:\n",
    "$\n",
    "\\|\\mathbf{v}\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = 5\n",
    "$\n",
    "$\n",
    "\\mathbf{u} = \\left[\\frac{3}{5}, \\frac{4}{5}\\right] = [0.6, 0.8]\n",
    "$\n",
    "\n",
    "📌 **Why is it useful?**  \n",
    "- Used in **cosine similarity** in NLP.  \n",
    "- Helps in **gradient computations** in ML.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73d625-ba01-422e-b212-eda8f04f3039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
