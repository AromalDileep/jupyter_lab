{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda42111-4404-4c37-88fe-275b3658145f",
   "metadata": {},
   "source": [
    "# Cost Function for Linear Regression\n",
    "\n",
    "The cost function $ J(\\beta) $ measures how well the linear regression model fits the data. It is defined as:\n",
    "\n",
    "$ J(\\beta) = \\frac{1}{2m} \\sum_{i=1}^{m} (Y^{(i)} - h_\\beta(X^{(i)}))^2 $\n",
    "\n",
    "Where:\n",
    "- $ J(\\beta) $: Cost function value (to be minimized)\n",
    "- $ m $: Number of data points (observations)\n",
    "- $ Y^{(i)} $: Actual value of the dependent variable for the $ i $-th data point\n",
    "- $ h_\\beta(X^{(i)}) $: Predicted value for the $ i $-th data point\n",
    "    $ h_\\beta(X^{(i)}) = \\beta_0 + \\beta_1 X_1^{(i)} + \\beta_2 X_2^{(i)} + ... + \\beta_p X_p^{(i)} $\n",
    "- $ \\beta $: Coefficients (parameters) of the linear regression model\n",
    "\n",
    "## Why Include the $ \\frac{1}{2} $ Factor?\n",
    "\n",
    "The factor $ \\frac{1}{2} $ is included for convenience because, during gradient descent, the derivative of the squared error results in a cancellation of this factor, simplifying the calculation.\n",
    "\n",
    "To see this, let's look at the derivative:\n",
    "\n",
    "$ \\frac{\\partial}{\\partial \\beta_j} (\\frac{1}{2}(Y^{(i)} - h_\\beta(X^{(i)}))^2) = (Y^{(i)} - h_\\beta(X^{(i)})) \\cdot \\frac{\\partial}{\\partial \\beta_j}(Y^{(i)} - h_\\beta(X^{(i)})) $\n",
    "\n",
    "The $ \\frac{1}{2} $ cancels with the 2 that comes from the power rule of differentiation.\n",
    "\n",
    "## Purpose of the Cost Function\n",
    "\n",
    "The cost function $ J(\\beta) $ quantifies the error between:\n",
    "- Predicted values: $ h_\\beta(X^{(i)}) $\n",
    "- Actual values: $ Y^{(i)} $\n",
    "\n",
    "Minimizing this cost function helps us find the optimal parameters $ \\beta $ for the best-fit line.\n",
    "\n",
    "## Matrix Form\n",
    "\n",
    "The cost function can also be written in matrix form:\n",
    "\n",
    "$ J(\\beta) = \\frac{1}{2m}(Y - X\\beta)^T(Y - X\\beta) $\n",
    "\n",
    "Where:\n",
    "- $ Y $ is the $ m \\times 1 $ vector of target values\n",
    "- $ X $ is the $ m \\times (p+1) $ matrix of features (including a column of ones for the intercept)\n",
    "- $ \\beta $ is the $ (p+1) \\times 1 $ vector of parameters\n",
    "\n",
    "## Gradient of the Cost Function\n",
    "\n",
    "The gradient with respect to each parameter $ \\beta_j $ is:\n",
    "\n",
    "$ \\frac{\\partial J}{\\partial \\beta_j} = -\\frac{1}{m} \\sum_{i=1}^{m} (Y^{(i)} - h_\\beta(X^{(i)}))X_j^{(i)} $\n",
    "\n",
    "In matrix form:\n",
    "$ \\nabla J(\\beta) = -\\frac{1}{m}X^T(Y - X\\beta) $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
