{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebac8df4-c011-4b72-9bfe-96cb2276b165",
   "metadata": {},
   "source": [
    "# Linear Regression and Gradient Descent\n",
    "\n",
    "## Linear Regression (Analytical Solution)\n",
    "Linear regression aims to find the best-fit line by minimizing the error between the predicted and actual values. The analytical approach involves directly calculating the parameters (weights) using matrix algebra.\n",
    "\n",
    "### 1. Problem Formulation\n",
    "- Model: $ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon $\n",
    "  - $ Y $: Dependent variable (target)\n",
    "  - $ X_i $: Independent variables (features)\n",
    "  - $ \\beta_0, \\beta_1, ..., \\beta_p $: Coefficients/parameters\n",
    "  - $ \\epsilon $: Error term\n",
    "\n",
    "### 2. Objective: Minimize the Cost Function\n",
    "The cost function for linear regression is the Mean Squared Error (MSE):\n",
    "\n",
    "$ J(\\beta) = \\frac{1}{2m} \\sum_{i=1}^{m} (Y^{(i)} - h_\\beta(X^{(i)}))^2 $\n",
    "\n",
    "- $ m $: Number of data points\n",
    "- $ h_\\beta(X^{(i)}) = \\beta_0 + \\sum_{j=1}^{p} \\beta_j X_j^{(i)} $: Prediction for the $ i $-th data point\n",
    "\n",
    "### 3. Analytical Solution Using the Normal Equation\n",
    "We solve the regression equation analytically using the Normal Equation:\n",
    "\n",
    "$ \\beta = (X^T X)^{-1} X^T Y $\n",
    "\n",
    "- $ X $: Feature matrix (with a column of 1s for the intercept)\n",
    "- $ Y $: Target vector\n",
    "- $ X^T X $: Covariance matrix\n",
    "- $ (X^T X)^{-1} $: Inverse of the covariance matrix\n",
    "\n",
    "**Steps**:\n",
    "1. Compute $ X^T X $\n",
    "2. Compute the inverse of $ X^T X $: $ (X^T X)^{-1} $\n",
    "3. Compute $ X^T Y $\n",
    "4. Multiply these matrices to get $ \\beta $\n",
    "\n",
    "### Why Gradient Descent Instead of the Analytical Solution?\n",
    "The analytical solution works well for small datasets, but:\n",
    "- For large datasets, inverting $ X^T X $ becomes computationally expensive\n",
    "- Gradient Descent provides an iterative, scalable approach that works for both small and large datasets\n",
    "\n",
    "## Gradient Descent for Linear Regression\n",
    "\n",
    "### 1. Initialize Parameters\n",
    "Set initial values for all coefficients $ \\beta_j $ (e.g., 0 or small random values):\n",
    "\n",
    "$ \\beta_0, \\beta_1, ..., \\beta_p $\n",
    "\n",
    "Choose:\n",
    "- Learning rate $ \\alpha $: A small value (e.g., 0.01 or 0.001) controlling step size\n",
    "- Number of iterations: A fixed number of steps or until convergence\n",
    "\n",
    "### 2. Compute Predictions\n",
    "For each data point $ i $, calculate the prediction $ h_\\beta(X^{(i)}) $:\n",
    "\n",
    "$ h_\\beta(X^{(i)}) = \\beta_0 + \\sum_{j=1}^{p} \\beta_j X_j^{(i)} $\n",
    "\n",
    "### 3. Calculate the Cost Function\n",
    "Evaluate the cost function $ J(\\beta) $:\n",
    "\n",
    "$ J(\\beta) = \\frac{1}{2m} \\sum_{i=1}^{m} (Y^{(i)} - h_\\beta(X^{(i)}))^2 $\n",
    "\n",
    "### 4. Compute the Gradient\n",
    "The gradient is the partial derivative of the cost function with respect to each parameter $ \\beta_j $:\n",
    "\n",
    "$ \\frac{\\partial J}{\\partial \\beta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\beta(X^{(i)}) - Y^{(i)}) X_j^{(i)} $\n",
    "\n",
    "### 5. Update Parameters\n",
    "Update each parameter $ \\beta_j $ using the gradient descent rule:\n",
    "\n",
    "$ \\beta_j = \\beta_j - \\alpha \\cdot \\frac{\\partial J}{\\partial \\beta_j} $\n",
    "\n",
    "Repeat this for all $ j $ (from 0 to $ p $):\n",
    "\n",
    "$ \\beta_0 = \\beta_0 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_\\beta(X^{(i)}) - Y^{(i)}) $\n",
    "$ \\beta_1 = \\beta_1 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_\\beta(X^{(i)}) - Y^{(i)}) X_1^{(i)} $\n",
    "$ \\beta_2 = \\beta_2 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_\\beta(X^{(i)}) - Y^{(i)}) X_2^{(i)} $\n",
    "\n",
    "### 6. Repeat Until Convergence\n",
    "Repeat steps 2–5 for a fixed number of iterations or until the parameters stabilize. Check:\n",
    "- The cost function $ J(\\beta) $ decreases consistently\n",
    "- The gradients $ \\frac{\\partial J}{\\partial \\beta_j} $ approach zero\n",
    "\n",
    "## Python Example: Linear Regression Using Gradient Descent\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Generate example data (simple linear regression)\n",
    "X = np.array([1, 2, 3, 4, 5])  # Features\n",
    "y = np.array([2, 4, 6, 8, 10])  # Labels\n",
    "\n",
    "# Add intercept term (column of 1s)\n",
    "X = np.c_[np.ones(X.shape[0]), X]  # X becomes [[1, x1], [1, x2], ...]\n",
    "\n",
    "# Initialize parameters\n",
    "theta = np.zeros(X.shape[1])  # [theta_0, theta_1]\n",
    "alpha = 0.01  # Learning rate\n",
    "iterations = 1000\n",
    "m = len(y)  # Number of data points\n",
    "\n",
    "# Gradient Descent\n",
    "for _ in range(iterations):\n",
    "    # Compute predictions\n",
    "    predictions = X.dot(theta)\n",
    "    \n",
    "    # Compute gradient\n",
    "    gradients = (1 / m) * X.T.dot(predictions - y)\n",
    "    \n",
    "    # Update parameters\n",
    "    theta -= alpha * gradients\n",
    "\n",
    "# Final parameters\n",
    "print(f\"Final parameters: {theta}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = X.dot(theta)\n",
    "print(f\"Predictions: {y_pred}\")\n",
    "```\n",
    "\n",
    "## Summary: Steps in Gradient Descent\n",
    "1. Initialize $ \\beta_j $ and set $ \\alpha $\n",
    "2. Compute predictions $ h_\\beta(X) $\n",
    "3. Calculate the cost function $ J(\\beta) $\n",
    "4. Compute gradients for $ \\beta_j $\n",
    "5. Update $ \\beta_j $ using the gradient descent rule\n",
    "6. Repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e8a8c-aabf-49ba-9119-f439c8994d4c",
   "metadata": {},
   "source": [
    "# Claude answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef69eecb-7fb3-41ed-aa23-04f5b2789e85",
   "metadata": {},
   "source": [
    "# Linear Regression: Step-by-Step Methods\n",
    "\n",
    "## Method 1: Direct Solution (Normal Equation)\n",
    "\n",
    "### Step 1: Set Up the Equation\n",
    "First, we write our linear regression equation:\n",
    "$ y = X\\beta + \\epsilon $\n",
    "\n",
    "Where:\n",
    "- $ y $ is the target variable (n×1 vector)\n",
    "- $ X $ is the feature matrix (n×p matrix, including a column of 1s for intercept)\n",
    "- $ \\beta $ is the coefficient vector (p×1 vector)\n",
    "- $ \\epsilon $ is the error term\n",
    "\n",
    "### Step 2: Derive the Normal Equation\n",
    "To find $ \\beta $ that minimizes the sum of squared errors:\n",
    "\n",
    "1. Write the sum of squared errors:\n",
    "   $ SSE = (y - X\\beta)^T(y - X\\beta) $\n",
    "\n",
    "2. Take derivative with respect to $ \\beta $ and set to zero:\n",
    "   $ \\frac{\\partial SSE}{\\partial \\beta} = -2X^T y + 2X^T X\\beta = 0 $\n",
    "\n",
    "3. Solve for $ \\beta $:\n",
    "   $ X^T X\\beta = X^T y $\n",
    "   $ \\beta = (X^T X)^{-1}X^T y $\n",
    "\n",
    "### Step 3: Solve Step by Step\n",
    "1. Calculate $ X^T X $:\n",
    "   - Multiply X transpose by X to get a p×p matrix\n",
    "\n",
    "2. Find $ (X^T X)^{-1} $:\n",
    "   - Calculate determinant of $ X^T X $\n",
    "   - Find adjugate matrix\n",
    "   - Divide adjugate by determinant\n",
    "\n",
    "3. Calculate $ X^T y $:\n",
    "   - Multiply X transpose by y vector\n",
    "\n",
    "4. Final multiplication:\n",
    "   - Multiply $ (X^T X)^{-1} $ by $ X^T y $\n",
    "\n",
    "### Example with 2D Data\n",
    "For simple linear regression ($ y = \\beta_0 + \\beta_1x $):\n",
    "\n",
    "$ X = \\begin{bmatrix}\n",
    "1 & x_1 \\\\\n",
    "1 & x_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_n\n",
    "\\end{bmatrix} $\n",
    "\n",
    "$ X^T X = \\begin{bmatrix}\n",
    "n & \\sum x_i \\\\\n",
    "\\sum x_i & \\sum x_i^2\n",
    "\\end{bmatrix} $\n",
    "\n",
    "$ X^T y = \\begin{bmatrix}\n",
    "\\sum y_i \\\\\n",
    "\\sum x_i y_i\n",
    "\\end{bmatrix} $\n",
    "\n",
    "## Method 2: Gradient Descent\n",
    "\n",
    "### Step 1: Initialize Parameters\n",
    "1. Choose initial values for $ \\beta $ (usually zeros)\n",
    "2. Set learning rate $ \\alpha $ (e.g., 0.01)\n",
    "3. Set maximum iterations and convergence threshold\n",
    "\n",
    "### Step 2: Forward Pass\n",
    "1. Calculate predictions:\n",
    "   $ \\hat{y} = X\\beta $\n",
    "\n",
    "2. Calculate error:\n",
    "   $ e = y - \\hat{y} $\n",
    "\n",
    "3. Calculate cost (MSE):\n",
    "   $ J = \\frac{1}{2n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $\n",
    "\n",
    "### Step 3: Backward Pass\n",
    "1. Calculate gradients for each parameter:\n",
    "   $ \\frac{\\partial J}{\\partial \\beta} = -\\frac{1}{n}X^T(y - X\\beta) $\n",
    "\n",
    "2. Update parameters:\n",
    "   $ \\beta_{new} = \\beta_{old} - \\alpha \\frac{\\partial J}{\\partial \\beta} $\n",
    "\n",
    "### Step 4: Iterate and Check Convergence\n",
    "1. Repeat Steps 2-3\n",
    "2. Stop if either:\n",
    "   - Maximum iterations reached\n",
    "   - $ |J_{new} - J_{old}| < \\text{threshold} $\n",
    "\n",
    "## Python Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, method='normal'):\n",
    "        self.method = method\n",
    "        self.coef_ = None\n",
    "        \n",
    "    def fit_normal(self, X, y):\n",
    "        # Add column of ones for intercept\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        \n",
    "        # Calculate beta using normal equation\n",
    "        XTX = X.T.dot(X)\n",
    "        XTy = X.T.dot(y)\n",
    "        self.coef_ = np.linalg.inv(XTX).dot(XTy)\n",
    "        \n",
    "    def fit_gradient_descent(self, X, y, learning_rate=0.01, max_iter=1000):\n",
    "        # Add column of ones for intercept\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.coef_ = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            # Forward pass\n",
    "            y_pred = X.dot(self.coef_)\n",
    "            \n",
    "            # Calculate gradients\n",
    "            gradients = -(1/n_samples) * X.T.dot(y - y_pred)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.coef_ -= learning_rate * gradients\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # Add column of ones for intercept\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        return X.dot(self.coef_)\n",
    "\n",
    "# Example usage:\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Using normal equation\n",
    "lr_normal = LinearRegression(method='normal')\n",
    "lr_normal.fit_normal(X, y)\n",
    "print(\"Normal Equation Coefficients:\", lr_normal.coef_)\n",
    "\n",
    "# Using gradient descent\n",
    "lr_gd = LinearRegression(method='gradient_descent')\n",
    "lr_gd.fit_gradient_descent(X, y)\n",
    "print(\"Gradient Descent Coefficients:\", lr_gd.coef_)\n",
    "```\n",
    "\n",
    "## Key Differences Between Methods\n",
    "\n",
    "### Normal Equation\n",
    "Advantages:\n",
    "- Direct solution (no iterations)\n",
    "- Exact solution\n",
    "- No learning rate to tune\n",
    "\n",
    "Disadvantages:\n",
    "- Computationally expensive for large datasets ($ O(n^3) $)\n",
    "- Can be numerically unstable\n",
    "- Requires matrix inversion\n",
    "\n",
    "### Gradient Descent\n",
    "Advantages:\n",
    "- Works well with large datasets\n",
    "- Memory efficient\n",
    "- Can be parallelized\n",
    "\n",
    "Disadvantages:\n",
    "- Requires learning rate tuning\n",
    "- May need many iterations\n",
    "- May converge to local minimum (not an issue for linear regression)\n",
    "- May not find exact solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202fc8f-fc51-4296-8da7-10d6fd75eaa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
