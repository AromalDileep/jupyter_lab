{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09e0756-33b4-4766-b5cf-010ba32eecac",
   "metadata": {},
   "source": [
    "Hereâ€™s a structured breakdown of **Regression**, its core concepts, and the various **Types of Regression** methods used in machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **Regression**  \n",
    "Regression is a **supervised machine learning technique** used for predicting continuous numerical values. The goal of regression is to establish a relationship between **independent variables (features)** and a **dependent variable (target)**.\n",
    "\n",
    "For example:  \n",
    "- Predicting house prices based on features like area, location, and number of rooms.  \n",
    "- Estimating stock prices based on historical trends.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Key Concepts in Regression**\n",
    "\n",
    "1. **Independent Variables (Predictors/Features)**: Input variables used to predict the outcome.  \n",
    "2. **Dependent Variable (Target)**: The continuous value you want to predict.  \n",
    "3. **Best Fit Line/Curve**: Regression aims to find the line or curve that best fits the data by minimizing errors.  \n",
    "4. **Error/Residual**: The difference between the actual value and the predicted value.  \n",
    "   \n",
    "   $\\text{Error} = y_{\\text{actual}} - y_{\\text{predicted}}$\n",
    "   \n",
    "5. **Loss Function**: A mathematical function to measure prediction errors (e.g., Mean Squared Error).  \n",
    "\n",
    "   \n",
    "  $ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$\n",
    " \n",
    "   where $( y_i )$ is the actual value and $( \\hat{y}_i )$ is the predicted value.  \n",
    "\n",
    "5. **Evaluation Metrics**: Metrics like RMSE, MAE, and $( R^2 )$-score to evaluate model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **Types of Regression**\n",
    "\n",
    "Here are the main types of regression techniques:\n",
    "\n",
    "### 1. **Linear Regression**  \n",
    "- Assumes a **linear relationship** between features and the target.  \n",
    "- Equation:  \n",
    "   $\n",
    "   y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n",
    "   $  \n",
    "   where $( y )$ is the target, $( x_i )$ are features, $( \\beta_i )$ are coefficients, and \\( \\epsilon \\) is the error term.  \n",
    "\n",
    "- **Types**:  \n",
    "   - **Simple Linear Regression** (one predictor)  \n",
    "   - **Multiple Linear Regression** (multiple predictors)  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Polynomial Regression**  \n",
    "- Used when the relationship between features and target is **non-linear** but can be approximated using a polynomial equation.  \n",
    "- Equation:  \n",
    "   $\n",
    "   y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_n x^n + \\epsilon\n",
    "     $\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Ridge Regression (L2 Regularization)**  \n",
    "- A regularized version of Linear Regression to handle **multicollinearity** and **overfitting**.  \n",
    "- Adds a penalty term proportional to the square of coefficients:  \n",
    "   $\n",
    "   \\text{Loss} = \\text{MSE} + \\lambda \\sum_{j=1}^n \\beta_j^2\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Lasso Regression (L1 Regularization)**  \n",
    "- Similar to Ridge but uses the **absolute value** of coefficients as a penalty, leading to **feature selection**.  \n",
    "   $\n",
    "   \\text{Loss} = \\text{MSE} + \\lambda \\sum_{j=1}^n |\\beta_j|\n",
    "   $ \n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Elastic Net Regression**  \n",
    "- A combination of Ridge (L2) and Lasso (L1) regularizations.  \n",
    "   $\n",
    "   \\text{Loss} = \\text{MSE} + \\lambda_1 \\sum_{j=1}^n |\\beta_j| + \\lambda_2 \\sum_{j=1}^n \\beta_j^2\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Logistic Regression**  \n",
    "- A regression technique for **binary classification problems**.  \n",
    "- Instead of predicting a continuous value, it predicts the **probability** of a class.  \n",
    "- Uses the **sigmoid function**:  \n",
    "   $\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}} \\quad \\text{where} \\, z = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\n",
    "   $  \n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Stepwise Regression**  \n",
    "- A method of building a regression model by **adding** or **removing features** based on statistical significance.  \n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Support Vector Regression (SVR)**  \n",
    "- Based on **Support Vector Machines (SVM)**, SVR uses hyperplanes to predict continuous values.  \n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Decision Tree Regression**  \n",
    "- Uses a **tree-like structure** to make decisions and predict continuous values.  \n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Random Forest Regression**  \n",
    "- An ensemble technique that builds multiple decision trees and averages the predictions.  \n",
    "\n",
    "---\n",
    "\n",
    "### 11. **Gradient Boosting Regression**  \n",
    "- A boosting ensemble method that builds models sequentially to correct previous errors.  \n",
    "\n",
    "---\n",
    "\n",
    "### 12. **Bayesian Regression**  \n",
    "- Incorporates Bayesian probability to estimate regression coefficients.  \n",
    "\n",
    "---\n",
    "\n",
    "### 13. **Principal Component Regression (PCR)**  \n",
    "- Uses **Principal Component Analysis (PCA)** for dimensionality reduction before applying regression.\n",
    "\n",
    "---\n",
    "\n",
    "### 14. **Quantile Regression**  \n",
    "- Predicts a specified **quantile** of the target distribution instead of the mean.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary Table**\n",
    "\n",
    "| **Type of Regression**           | **Key Characteristics**                                |\n",
    "|----------------------------------|-------------------------------------------------------|\n",
    "| Linear Regression                | Assumes linear relationship between features and target. |\n",
    "| Polynomial Regression            | Handles non-linear relationships using polynomials.   |\n",
    "| Ridge Regression                 | L2 regularization to reduce overfitting.              |\n",
    "| Lasso Regression                 | L1 regularization; performs feature selection.        |\n",
    "| Elastic Net                      | Combines Ridge and Lasso penalties.                   |\n",
    "| Logistic Regression              | For binary classification problems.                   |\n",
    "| Support Vector Regression (SVR)  | Uses hyperplanes to predict continuous outcomes.      |\n",
    "| Decision Tree Regression         | Tree-based model for regression.                      |\n",
    "| Random Forest Regression         | Ensemble of decision trees for better accuracy.       |\n",
    "| Gradient Boosting Regression     | Sequential boosting approach to reduce errors.        |\n",
    "| Bayesian Regression              | Bayesian approach to regression.                      |\n",
    "| Principal Component Regression   | Applies PCA before regression for dimensionality reduction. |\n",
    "| Quantile Regression              | Predicts quantiles instead of mean values.            |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
